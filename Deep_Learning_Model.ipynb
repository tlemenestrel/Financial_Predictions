{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Paper on LSTM Neural Networks for stock prices volatility prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs230.stanford.edu/projects_fall_2019/reports/26254244.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial for building an LSTM neural network for time-series prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas_datareader import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Datetime\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Scikit-Learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# GARCH model\n",
    "\n",
    "import pyflux as pf\n",
    "\n",
    "# Keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the csv file with the financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19900611  19900612  19900613  19900614  19900615  19900618  19900619  \\\n",
      "0    358.71    361.63    366.25    364.90    362.90    362.91    356.88   \n",
      "1    361.63    366.25    364.90    362.90    362.91    356.88    358.47   \n",
      "2    361.63    367.27    367.09    364.90    363.14    362.91    358.90   \n",
      "3    357.70    361.15    364.51    361.64    360.71    356.88    356.18   \n",
      "4      0.00      0.00      0.00      0.00      0.00      0.00      0.00   \n",
      "\n",
      "   19900620  19900621  19900622    ...     20200528  20200529  20200601  \\\n",
      "0    358.47    359.10    360.47    ...      3046.61   3025.17   3038.78   \n",
      "1    359.10    360.47    355.43    ...      3029.73   3044.31   3055.73   \n",
      "2    359.91    360.88    363.20    ...      3068.67   3049.17   3062.18   \n",
      "3    357.00    357.63    355.31    ...      3023.40   2998.61   3031.54   \n",
      "4      0.00      0.00      0.00    ...         0.00      0.00      0.00   \n",
      "\n",
      "   20200602  20200603  20200604  20200605  20200608  20200609  20200610  \n",
      "0   3064.78   3098.90   3111.56   3163.84   3199.92   3213.32   3213.42  \n",
      "1   3080.82   3122.87   3112.35   3193.93   3232.39   3207.18   3190.14  \n",
      "2   3081.07   3130.94   3128.91   3211.72   3233.13   3222.71   3223.27  \n",
      "3   3051.64   3098.90   3090.41   3163.84   3196.00   3193.11   3181.49  \n",
      "4      0.00      0.00      0.00      0.00      0.00      0.00      0.00  \n",
      "\n",
      "[5 rows x 7559 columns]\n",
      "(10, 7559)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'input/financial_data.csv')\n",
    "\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, we have 254 columns, corresponding to the 254 business days for which we have financial data and 10 columns, which are the 10 financial indicators we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposing the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are working with time series data, we should have the dates as one column and will thus use transpose() for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0       1       2       3    4    5    6    7    8    9\n",
      "19900611  358.71  361.63  361.63  357.70  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "19900612  361.63  366.25  367.27  361.15  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "19900613  366.25  364.90  367.09  364.51  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "19900614  364.90  362.90  364.90  361.64  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "19900615  362.90  362.91  363.14  360.71  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "(7559, 10)\n"
     ]
    }
   ],
   "source": [
    "df = df.transpose()\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the index of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index       0       1       2       3    4    5    6    7    8    9\n",
      "0  19900611  358.71  361.63  361.63  357.70  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "1  19900612  361.63  366.25  367.27  361.15  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2  19900613  366.25  364.90  367.09  364.51  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "3  19900614  364.90  362.90  364.90  361.64  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "4  19900615  362.90  362.91  363.14  360.71  0.0  0.0  0.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the columns with the financial indicators name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date    Open   Close    High     Low  Volume  RSI14  SMA14  EMA14  \\\n",
      "0  19900611  358.71  361.63  361.63  357.70     0.0    0.0    0.0    0.0   \n",
      "1  19900612  361.63  366.25  367.27  361.15     0.0    0.0    0.0    0.0   \n",
      "2  19900613  366.25  364.90  367.09  364.51     0.0    0.0    0.0    0.0   \n",
      "3  19900614  364.90  362.90  364.90  361.64     0.0    0.0    0.0    0.0   \n",
      "4  19900615  362.90  362.91  363.14  360.71     0.0    0.0    0.0    0.0   \n",
      "\n",
      "   MACD_sl  MACD_h  \n",
      "0      0.0     0.0  \n",
      "1      0.0     0.0  \n",
      "2      0.0     0.0  \n",
      "3      0.0     0.0  \n",
      "4      0.0     0.0  \n"
     ]
    }
   ],
   "source": [
    "df = df.rename(columns={\n",
    "    \n",
    "    df.columns[0]: 'Date',\n",
    "    df.columns[1]:'Open',\n",
    "    df.columns[2]: 'Close',\n",
    "    df.columns[3]:'High',\n",
    "    df.columns[4]:'Low',\n",
    "    df.columns[5]: 'Volume',\n",
    "    df.columns[6]: 'RSI14',\n",
    "    df.columns[7]:'SMA14',\n",
    "    df.columns[8]: 'EMA14',\n",
    "    df.columns[9]:'MACD_sl',\n",
    "    df.columns[10]:'MACD_h'\n",
    "\n",
    "})\n",
    "\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Date column into a Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] =  pd.to_datetime(df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Date column as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open   Close    High     Low  Volume  RSI14  SMA14  EMA14  \\\n",
      "Date                                                                      \n",
      "1990-06-11  358.71  361.63  361.63  357.70     0.0    0.0    0.0    0.0   \n",
      "1990-06-12  361.63  366.25  367.27  361.15     0.0    0.0    0.0    0.0   \n",
      "1990-06-13  366.25  364.90  367.09  364.51     0.0    0.0    0.0    0.0   \n",
      "1990-06-14  364.90  362.90  364.90  361.64     0.0    0.0    0.0    0.0   \n",
      "1990-06-15  362.90  362.91  363.14  360.71     0.0    0.0    0.0    0.0   \n",
      "\n",
      "            MACD_sl  MACD_h  \n",
      "Date                         \n",
      "1990-06-11      0.0     0.0  \n",
      "1990-06-12      0.0     0.0  \n",
      "1990-06-13      0.0     0.0  \n",
      "1990-06-14      0.0     0.0  \n",
      "1990-06-15      0.0     0.0  \n"
     ]
    }
   ],
   "source": [
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding volume from Yahoo Finance API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1990-06-11'\n",
    "end_date = '2020-06-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_data = data.DataReader('^GSPC', 'yahoo', start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  High         Low        Open       Close     Volume  \\\n",
      "Date                                                                    \n",
      "1990-06-11  361.630005  357.700012  358.709991  361.630005  119550000   \n",
      "1990-06-12  367.269989  361.149994  361.630005  366.250000  157100000   \n",
      "1990-06-13  367.089996  364.510010  366.250000  364.899994  158910000   \n",
      "1990-06-14  364.899994  361.640015  364.899994  362.899994  135770000   \n",
      "1990-06-15  363.140015  360.709991  362.890015  362.910004  205130000   \n",
      "\n",
      "             Adj Close  \n",
      "Date                    \n",
      "1990-06-11  361.630005  \n",
      "1990-06-12  366.250000  \n",
      "1990-06-13  364.899994  \n",
      "1990-06-14  362.899994  \n",
      "1990-06-15  362.910004  \n",
      "(7559, 6)\n"
     ]
    }
   ],
   "source": [
    "print (panel_data.head())\n",
    "\n",
    "print( panel_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Volume'] =  panel_data['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print out the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_columns_to_dropna(df, column_list):\n",
    "    \n",
    "    for column in column_list:\n",
    "        \n",
    "        df = df[df[column].notna()]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open   Close    High     Low     Volume  RSI14  SMA14  EMA14  \\\n",
      "Date                                                                         \n",
      "1990-06-11  358.71  361.63  361.63  357.70  119550000    0.0    0.0    0.0   \n",
      "1990-06-12  361.63  366.25  367.27  361.15  157100000    0.0    0.0    0.0   \n",
      "1990-06-13  366.25  364.90  367.09  364.51  158910000    0.0    0.0    0.0   \n",
      "1990-06-14  364.90  362.90  364.90  361.64  135770000    0.0    0.0    0.0   \n",
      "1990-06-15  362.90  362.91  363.14  360.71  205130000    0.0    0.0    0.0   \n",
      "\n",
      "            MACD_sl  MACD_h  \n",
      "Date                         \n",
      "1990-06-11      0.0     0.0  \n",
      "1990-06-12      0.0     0.0  \n",
      "1990-06-13      0.0     0.0  \n",
      "1990-06-14      0.0     0.0  \n",
      "1990-06-15      0.0     0.0  \n",
      "(7559, 10)\n"
     ]
    }
   ],
   "source": [
    "column_list = ['Open','Close']\n",
    "\n",
    "df = list_columns_to_dropna(df, column_list)\n",
    "\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_type_of_dataframe_columns(df):\n",
    "    \n",
    "    dataTypeSeries = df.dtypes\n",
    " \n",
    "    print('Data type of each column of Dataframe :')\n",
    "    print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open   Close    High     Low     Volume  RSI14  SMA14  EMA14  \\\n",
      "Date                                                                         \n",
      "1990-06-11  358.71  361.63  361.63  357.70  119550000    0.0    0.0    0.0   \n",
      "1990-06-12  361.63  366.25  367.27  361.15  157100000    0.0    0.0    0.0   \n",
      "1990-06-13  366.25  364.90  367.09  364.51  158910000    0.0    0.0    0.0   \n",
      "1990-06-14  364.90  362.90  364.90  361.64  135770000    0.0    0.0    0.0   \n",
      "1990-06-15  362.90  362.91  363.14  360.71  205130000    0.0    0.0    0.0   \n",
      "\n",
      "            MACD_sl  MACD_h  Log_Returns  \n",
      "Date                                      \n",
      "1990-06-11      0.0     0.0          NaN  \n",
      "1990-06-12      0.0     0.0     0.012695  \n",
      "1990-06-13      0.0     0.0    -0.003693  \n",
      "1990-06-14      0.0     0.0    -0.005496  \n",
      "1990-06-15      0.0     0.0     0.000028  \n",
      "               Open    Close     High      Low      Volume  RSI14    SMA14  \\\n",
      "Date                                                                         \n",
      "2020-06-04  3111.56  3112.35  3128.91  3090.41  6428130000  82.33  3006.42   \n",
      "2020-06-05  3163.84  3193.93  3211.72  3163.84  8617590000  84.97  3030.00   \n",
      "2020-06-08  3199.92  3232.39  3233.13  3196.00  8437380000  83.12  3049.90   \n",
      "2020-06-09  3213.32  3207.18  3222.71  3193.11  6382620000  84.27  3070.20   \n",
      "2020-06-10  3213.42  3190.14  3223.27  3181.49  6570840000  78.52  3085.81   \n",
      "\n",
      "              EMA14  MACD_sl  MACD_h  Log_Returns  \n",
      "Date                                               \n",
      "2020-06-04  3019.03    58.68   12.44    -0.003374  \n",
      "2020-06-05  3042.35    62.71   16.10     0.025874  \n",
      "2020-06-08  3067.69    67.57   19.43     0.011970  \n",
      "2020-06-09  3086.29    72.14   18.28    -0.007830  \n",
      "2020-06-10  3100.14    75.85   14.85    -0.005327  \n"
     ]
    }
   ],
   "source": [
    "df['Log_Returns'] = np.log(df.Close) - np.log(df.Close.shift(1))\n",
    "\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Trading Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open   Close    High     Low     Volume  RSI14  SMA14  EMA14  \\\n",
      "Date                                                                         \n",
      "1990-06-11  358.71  361.63  361.63  357.70  119550000    0.0    0.0    0.0   \n",
      "1990-06-12  361.63  366.25  367.27  361.15  157100000    0.0    0.0    0.0   \n",
      "1990-06-13  366.25  364.90  367.09  364.51  158910000    0.0    0.0    0.0   \n",
      "1990-06-14  364.90  362.90  364.90  361.64  135770000    0.0    0.0    0.0   \n",
      "1990-06-15  362.90  362.91  363.14  360.71  205130000    0.0    0.0    0.0   \n",
      "\n",
      "            MACD_sl  MACD_h  Log_Returns  Log_Trading_Range  \n",
      "Date                                                         \n",
      "1990-06-11      0.0     0.0          NaN           0.010927  \n",
      "1990-06-12      0.0     0.0     0.012695           0.016804  \n",
      "1990-06-13      0.0     0.0    -0.003693           0.007053  \n",
      "1990-06-14      0.0     0.0    -0.005496           0.008974  \n",
      "1990-06-15      0.0     0.0     0.000028           0.006714  \n"
     ]
    }
   ],
   "source": [
    "df['Log_Trading_Range'] = np.log(df.High) - np.log(df.Low)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Volume Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open   Close    High     Low     Volume  RSI14  SMA14  EMA14  \\\n",
      "Date                                                                         \n",
      "1990-06-11  358.71  361.63  361.63  357.70  119550000    0.0    0.0    0.0   \n",
      "1990-06-12  361.63  366.25  367.27  361.15  157100000    0.0    0.0    0.0   \n",
      "1990-06-13  366.25  364.90  367.09  364.51  158910000    0.0    0.0    0.0   \n",
      "1990-06-14  364.90  362.90  364.90  361.64  135770000    0.0    0.0    0.0   \n",
      "1990-06-15  362.90  362.91  363.14  360.71  205130000    0.0    0.0    0.0   \n",
      "\n",
      "            MACD_sl  MACD_h  Log_Returns  Log_Trading_Range  Log_Volume_Change  \n",
      "Date                                                                            \n",
      "1990-06-11      0.0     0.0          NaN           0.010927                NaN  \n",
      "1990-06-12      0.0     0.0     0.012695           0.016804           0.273148  \n",
      "1990-06-13      0.0     0.0    -0.003693           0.007053           0.011455  \n",
      "1990-06-14      0.0     0.0    -0.005496           0.008974          -0.157376  \n",
      "1990-06-15      0.0     0.0     0.000028           0.006714           0.412682  \n"
     ]
    }
   ],
   "source": [
    "df['Log_Volume_Change'] = np.log(df.Volume) - np.log(df.Volume.shift(1))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous 10-day Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Open    Close     High      Low      Volume  RSI14    SMA14  \\\n",
      "Date                                                                         \n",
      "2020-06-04  3111.56  3112.35  3128.91  3090.41  6428130000  82.33  3006.42   \n",
      "2020-06-05  3163.84  3193.93  3211.72  3163.84  8617590000  84.97  3030.00   \n",
      "2020-06-08  3199.92  3232.39  3233.13  3196.00  8437380000  83.12  3049.90   \n",
      "2020-06-09  3213.32  3207.18  3222.71  3193.11  6382620000  84.27  3070.20   \n",
      "2020-06-10  3213.42  3190.14  3223.27  3181.49  6570840000  78.52  3085.81   \n",
      "\n",
      "              EMA14  MACD_sl  MACD_h  Log_Returns  Log_Trading_Range  \\\n",
      "Date                                                                   \n",
      "2020-06-04  3019.03    58.68   12.44    -0.003374           0.012381   \n",
      "2020-06-05  3042.35    62.71   16.10     0.025874           0.015020   \n",
      "2020-06-08  3067.69    67.57   19.43     0.011970           0.011551   \n",
      "2020-06-09  3086.29    72.14   18.28    -0.007830           0.009227   \n",
      "2020-06-10  3100.14    75.85   14.85    -0.005327           0.013047   \n",
      "\n",
      "            Log_Volume_Change  Previous_10_Day_Volatility  \n",
      "Date                                                       \n",
      "2020-06-04           0.070666                    0.007613  \n",
      "2020-06-05           0.293122                    0.008851  \n",
      "2020-06-08          -0.021134                    0.008691  \n",
      "2020-06-09          -0.279093                    0.010060  \n",
      "2020-06-10           0.029063                    0.010334  \n"
     ]
    }
   ],
   "source": [
    "df['Previous_10_Day_Volatility'] = df['Log_Returns'].rolling(window = 10).std()\n",
    "\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous 30-day Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open   Close    High     Low     Volume  RSI14  SMA14  EMA14  \\\n",
      "Date                                                                         \n",
      "1990-06-11  358.71  361.63  361.63  357.70  119550000    0.0    0.0    0.0   \n",
      "1990-06-12  361.63  366.25  367.27  361.15  157100000    0.0    0.0    0.0   \n",
      "1990-06-13  366.25  364.90  367.09  364.51  158910000    0.0    0.0    0.0   \n",
      "1990-06-14  364.90  362.90  364.90  361.64  135770000    0.0    0.0    0.0   \n",
      "1990-06-15  362.90  362.91  363.14  360.71  205130000    0.0    0.0    0.0   \n",
      "\n",
      "            MACD_sl  MACD_h  Log_Returns  Log_Trading_Range  \\\n",
      "Date                                                          \n",
      "1990-06-11      0.0     0.0          NaN           0.010927   \n",
      "1990-06-12      0.0     0.0     0.012695           0.016804   \n",
      "1990-06-13      0.0     0.0    -0.003693           0.007053   \n",
      "1990-06-14      0.0     0.0    -0.005496           0.008974   \n",
      "1990-06-15      0.0     0.0     0.000028           0.006714   \n",
      "\n",
      "            Log_Volume_Change  Previous_10_Day_Volatility  \\\n",
      "Date                                                        \n",
      "1990-06-11                NaN                         NaN   \n",
      "1990-06-12           0.273148                         NaN   \n",
      "1990-06-13           0.011455                         NaN   \n",
      "1990-06-14          -0.157376                         NaN   \n",
      "1990-06-15           0.412682                         NaN   \n",
      "\n",
      "            Previous_30_Day_Volatility  \n",
      "Date                                    \n",
      "1990-06-11                         NaN  \n",
      "1990-06-12                         NaN  \n",
      "1990-06-13                         NaN  \n",
      "1990-06-14                         NaN  \n",
      "1990-06-15                         NaN  \n"
     ]
    }
   ],
   "source": [
    "df['Previous_30_Day_Volatility'] = df['Log_Returns'].rolling(window = 30).std()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next 10-days volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open   Close    High     Low     Volume  RSI14  SMA14  EMA14  \\\n",
      "Date                                                                         \n",
      "1990-06-11  358.71  361.63  361.63  357.70  119550000    0.0    0.0    0.0   \n",
      "1990-06-12  361.63  366.25  367.27  361.15  157100000    0.0    0.0    0.0   \n",
      "1990-06-13  366.25  364.90  367.09  364.51  158910000    0.0    0.0    0.0   \n",
      "1990-06-14  364.90  362.90  364.90  361.64  135770000    0.0    0.0    0.0   \n",
      "1990-06-15  362.90  362.91  363.14  360.71  205130000    0.0    0.0    0.0   \n",
      "\n",
      "            MACD_sl  MACD_h  Log_Returns  Log_Trading_Range  \\\n",
      "Date                                                          \n",
      "1990-06-11      0.0     0.0          NaN           0.010927   \n",
      "1990-06-12      0.0     0.0     0.012695           0.016804   \n",
      "1990-06-13      0.0     0.0    -0.003693           0.007053   \n",
      "1990-06-14      0.0     0.0    -0.005496           0.008974   \n",
      "1990-06-15      0.0     0.0     0.000028           0.006714   \n",
      "\n",
      "            Log_Volume_Change  Previous_10_Day_Volatility  \\\n",
      "Date                                                        \n",
      "1990-06-11                NaN                         NaN   \n",
      "1990-06-12           0.273148                         NaN   \n",
      "1990-06-13           0.011455                         NaN   \n",
      "1990-06-14          -0.157376                         NaN   \n",
      "1990-06-15           0.412682                         NaN   \n",
      "\n",
      "            Previous_30_Day_Volatility  Next_10_Days_Volatility  \n",
      "Date                                                             \n",
      "1990-06-11                         NaN                      NaN  \n",
      "1990-06-12                         NaN                 0.009012  \n",
      "1990-06-13                         NaN                 0.007320  \n",
      "1990-06-14                         NaN                 0.008348  \n",
      "1990-06-15                         NaN                 0.008806  \n"
     ]
    }
   ],
   "source": [
    "df['Next_10_Days_Volatility'] = df['Log_Returns'].iloc[::-1].rolling(window = 10).std().iloc[::-1]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'output/output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GARCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Garch predictions for the entire dataset of SPX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a new dataframe for splitting the dataframe in test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Open    Close     High      Low      Volume  RSI14    SMA14  \\\n",
      "Date                                                                         \n",
      "2016-04-13  2065.92  2082.42  2083.18  2065.92  4191830000  63.32  2055.58   \n",
      "2016-04-14  2082.89  2082.78  2087.84  2078.13  3765870000  63.68  2058.92   \n",
      "2016-04-15  2083.10  2080.73  2083.22  2076.31  3701450000  62.69  2062.04   \n",
      "2016-04-18  2078.83  2094.34  2094.66  2073.65  3316880000  61.72  2064.85   \n",
      "2016-04-19  2096.05  2100.80  2104.05  2091.68  3896830000  61.15  2067.48   \n",
      "\n",
      "              EMA14  MACD_sl  MACD_h  Log_Returns  Log_Trading_Range  \\\n",
      "Date                                                                   \n",
      "2016-04-13  2052.77    21.99   -2.44     0.009990           0.008320   \n",
      "2016-04-14  2056.77    21.65   -1.35     0.000173           0.004662   \n",
      "2016-04-15  2059.96    21.42   -0.92    -0.000985           0.003322   \n",
      "2016-04-18  2064.55    21.44    0.07     0.006520           0.010081   \n",
      "2016-04-19  2069.38    21.67    0.90     0.003080           0.005896   \n",
      "\n",
      "            Log_Volume_Change  Previous_10_Day_Volatility  \\\n",
      "Date                                                        \n",
      "2016-04-13          -0.011365                    0.008248   \n",
      "2016-04-14          -0.107158                    0.008190   \n",
      "2016-04-15          -0.017254                    0.007998   \n",
      "2016-04-18          -0.109700                    0.008104   \n",
      "2016-04-19           0.161139                    0.007015   \n",
      "\n",
      "            Previous_30_Day_Volatility  Next_10_Days_Volatility  \n",
      "Date                                                             \n",
      "2016-04-13                    0.006509                 0.004300  \n",
      "2016-04-14                    0.006499                 0.003100  \n",
      "2016-04-15                    0.006505                 0.004410  \n",
      "2016-04-18                    0.006563                 0.004660  \n",
      "2016-04-19                    0.006568                 0.004889  \n"
     ]
    }
   ],
   "source": [
    "X = df[df.first_valid_index():df.last_valid_index()- datetime.timedelta(1500)]\n",
    "\n",
    "print (X.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a GARCH model using PyFlux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GARCH(1,1)                                                                                                \n",
      "======================================================= ==================================================\n",
      "Dependent Variable: Log_Returns                         Method: MLE                                       \n",
      "Start Date: 1990-07-25 00:00:00                         Log Likelihood: 21134.2564                        \n",
      "End Date: 2016-04-19 00:00:00                           AIC: -42260.5128                                  \n",
      "Number of observations: 6485                            BIC: -42233.4038                                  \n",
      "==========================================================================================================\n",
      "Latent Variable                          Estimate   Std Error  z        P>|z|    95% C.I.                 \n",
      "======================================== ========== ========== ======== ======== =========================\n",
      "Vol Constant                             0.0                                                              \n",
      "q(1)                                     0.0903                                                           \n",
      "p(1)                                     0.8975                                                           \n",
      "Returns Constant                         0.0005     0.0039     0.1384   0.8899   (-0.0071 | 0.0082)       \n",
      "==========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "GARCH_model = pf.GARCH(X, target = 'Log_Returns', p=1, q=1)\n",
    "\n",
    "x = GARCH_model.fit()\n",
    "\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making rolling predictions using the GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-602-50fa57aed184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGARCH_rolling_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGARCH_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_is\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_once\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGARCH_rolling_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pyflux/garch/garch.py\u001b[0m in \u001b[0;36mpredict_is\u001b[0;34m(self, h, fit_once, fit_method, intervals, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfit_once\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_lvs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintervals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintervals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mintervals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pyflux/garch/garch.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, h, intervals)\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0msim_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sim_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m                 \u001b[0mmean_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sim_predicted_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m                 \u001b[0mforecasted_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pyflux/garch/garch.py\u001b[0m in \u001b[0;36m_sim_predicted_mean\u001b[0;34m(self, sigma2, Y, scores, h, t_params, simulations)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m                         \u001b[0mnew_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma2_exp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GARCH_rolling_predictions = GARCH_model.predict_is(h = len(X) - 50, fit_once = True)\n",
    "\n",
    "print(GARCH_rolling_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making forward-looking predictions using the GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_forward_looking_predictions = GARCH_model.predict(h=1500)\n",
    "\n",
    "print(GARCH_forward_looking_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming one of the columns of the GARCH Model Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_rolling_predictions.rename(columns={'Series':'GARCH_rolling_predictions'}, inplace =True)\n",
    "\n",
    "GARCH_forward_looking_predictions.rename(columns={'Log_Returns':'GARCH_forward_looking_predictions'}, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the new feature to the current dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, GARCH_rolling_predictions], axis=1)\n",
    "df = pd.concat([df, GARCH_forward_looking_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Nan values with 0s for the GARCH Predictions columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rational for this from Keras's creator:\n",
    "\n",
    "https://stackoverflow.com/questions/52570199/multivariate-lstm-with-missing-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GARCH_forward_looking_predictions'] =  df['GARCH_forward_looking_predictions'].fillna(0)\n",
    "df['GARCH_rolling_predictions'] =  df['GARCH_rolling_predictions'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the results of our transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to predict VIX prices than realized volatility of SPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use it to predict VIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at relationship of (5, 10, 30) realized volatility of SPX versus VIX prices (Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate in Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a new dataframe for splitting the dataframe in test and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dropna on several columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_columns_to_dropna(df, column_list):\n",
    "    \n",
    "    for column in column_list:\n",
    "        \n",
    "        df = df[df[column].notna()]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = ['Open', 'Log_Returns','Previous_10_Day_Volatility','Next_10_Days_Volatility','Previous_30_Day_Volatility']\n",
    "\n",
    "df = list_columns_to_dropna(df, column_list)\n",
    "\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the final dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'output/output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the S&P 500 Prices from 1990 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].plot(label = 'S&P 500', figsize =(16,8), title = 'S&P 500 Stock Prices from 1990 to 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the 10-days forward looking volatility of  S&P 500 Prices from 1990 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Next_10_Days_Volatility'].plot(label = 'S&P 500', figsize =(16,8), title = '10-days forward looking volatility of  S&P 500 Prices from 1990 to 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pearson_correlation_matrix_of_dataframe(size_x,size_y,dataframe,correlation_target,correlation_minimum_criteria):\n",
    "\n",
    "    # Using Pearson Correlation\n",
    "\n",
    "    plt.figure(figsize=(size_x,size_y))\n",
    "    cor = dataframe.corr()\n",
    "    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "    plt.savefig('Images/pearson_correlation_matrix.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation with output variable\n",
    "\n",
    "    target = abs(cor[correlation_target])\n",
    "\n",
    "    #Selecting and printing highly correlated features\n",
    "\n",
    "    relevant_features = target[target>correlation_minimum_criteria]\n",
    "    print(relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_pearson_correlation_matrix_of_dataframe(20,20,df,\"Next_10_Days_Volatility\",0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6020, 5) (6020, 1)\n",
      "(1500, 5) (1500, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df.drop([\"Next_10_Days_Volatility\",'Low','High','Close','Open','Volume','MACD_h','MACD_sl','RSI14','SMA14','EMA14'], axis=1).values)\n",
    "y = np.array(df[\"Next_10_Days_Volatility\"].values).reshape(-1, 1) \n",
    "\n",
    "test_size = 1500\n",
    "\n",
    "X_train = X[test_size:,]\n",
    "X_test = X[:test_size,]\n",
    "y_train = y[test_size:]\n",
    "y_test = y[:test_size]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to get lagged versions of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function increases the number of features of the dataset by \"lagging\" every feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lagged(x, y, t, s):\n",
    "    \n",
    "    lagged = []\n",
    "    \n",
    "    for i in range(x.shape[0] - t):\n",
    "        \n",
    "        if i == x.shape[0] - t:\n",
    "            \n",
    "            break\n",
    "            \n",
    "        for k in range(t):\n",
    "            \n",
    "            if k < t:\n",
    "                \n",
    "                lagged.append(x[i+k])\n",
    "                \n",
    "    lagged = np.array(lagged).reshape(s)\n",
    "    \n",
    "    return lagged, y[:lagged.shape[0],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5990, 150) (1470, 150)\n",
      "(5990, 1) (1470, 1)\n"
     ]
    }
   ],
   "source": [
    "N = 30\n",
    "\n",
    "X_train, y_train = get_lagged(X_train, y_train, N, (X_train.shape[0]-N, N*X_train.shape[1]))\n",
    "X_test, y_test = get_lagged(X_test, y_test, N, (X_test.shape[0]-N, N*X_test.shape[1]))\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5986, 4, 150) (1466, 4, 150)\n",
      "(5986, 1) (1466, 1)\n"
     ]
    }
   ],
   "source": [
    "T = 4\n",
    "\n",
    "X_train, y_train = get_lagged(X_train, y_train, T, (X_train.shape[0]-T, T, X_train.shape[1]))\n",
    "X_test, y_test = get_lagged(X_test, y_test, T, (X_test.shape[0]-T, T, X_test.shape[1]))\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 4, 150)]          0         \n",
      "_________________________________________________________________\n",
      "lstm_82 (LSTM)               (None, 4, 200)            280800    \n",
      "_________________________________________________________________\n",
      "lstm_83 (LSTM)               (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 601,801\n",
      "Trainable params: 601,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputLSTM = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "y = LSTM(200, return_sequences=True)(inputLSTM)\n",
    "y = LSTM(200)(y)\n",
    "y = Dense(1)(y)\n",
    "lstm = Model(inputs=inputLSTM, outputs=y)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(lstm, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the parameters of the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(optimizer=keras.optimizers.Adam(lr=0.01),loss=tf.keras.losses.MeanSquaredError(),metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data for SPX where you have Open Close and Volumen (1960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "6/6 [==============================] - 1s 193ms/step - loss: 0.0089 - root_mean_squared_error: 0.0945 - val_loss: 3.1353e-04 - val_root_mean_squared_error: 0.0177\n",
      "Epoch 2/60\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 0.0011 - root_mean_squared_error: 0.0336 - val_loss: 7.7163e-04 - val_root_mean_squared_error: 0.0278\n",
      "Epoch 3/60\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.0010 - root_mean_squared_error: 0.0317 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0342\n",
      "Epoch 4/60\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 7.2303e-04 - root_mean_squared_error: 0.0269 - val_loss: 1.4526e-04 - val_root_mean_squared_error: 0.0121\n",
      "Epoch 5/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 2.1186e-04 - root_mean_squared_error: 0.0146 - val_loss: 7.0001e-05 - val_root_mean_squared_error: 0.0084\n",
      "Epoch 6/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 1.3382e-04 - root_mean_squared_error: 0.0116 - val_loss: 5.0957e-05 - val_root_mean_squared_error: 0.0071\n",
      "Epoch 7/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 7.6547e-05 - root_mean_squared_error: 0.0087 - val_loss: 2.8005e-05 - val_root_mean_squared_error: 0.0053\n",
      "Epoch 8/60\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 4.7136e-05 - root_mean_squared_error: 0.0069 - val_loss: 2.4258e-05 - val_root_mean_squared_error: 0.0049\n",
      "Epoch 9/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 2.9058e-05 - root_mean_squared_error: 0.0054 - val_loss: 1.8997e-05 - val_root_mean_squared_error: 0.0044\n",
      "Epoch 10/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 1.8155e-05 - root_mean_squared_error: 0.0043 - val_loss: 1.8137e-05 - val_root_mean_squared_error: 0.0043\n",
      "Epoch 11/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 1.1294e-05 - root_mean_squared_error: 0.0034 - val_loss: 1.5719e-05 - val_root_mean_squared_error: 0.0040\n",
      "Epoch 12/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 1.1257e-05 - root_mean_squared_error: 0.0034 - val_loss: 1.4511e-05 - val_root_mean_squared_error: 0.0038\n",
      "Epoch 13/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 8.8333e-06 - root_mean_squared_error: 0.0030 - val_loss: 1.1522e-05 - val_root_mean_squared_error: 0.0034\n",
      "Epoch 14/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 8.1344e-06 - root_mean_squared_error: 0.0029 - val_loss: 1.1223e-05 - val_root_mean_squared_error: 0.0034\n",
      "Epoch 15/60\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 7.4723e-06 - root_mean_squared_error: 0.0027 - val_loss: 1.0644e-05 - val_root_mean_squared_error: 0.0033\n",
      "Epoch 16/60\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 6.8666e-06 - root_mean_squared_error: 0.0026 - val_loss: 9.8417e-06 - val_root_mean_squared_error: 0.0031\n",
      "Epoch 17/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 6.7505e-06 - root_mean_squared_error: 0.0026 - val_loss: 9.8672e-06 - val_root_mean_squared_error: 0.0031\n",
      "Epoch 18/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 6.2938e-06 - root_mean_squared_error: 0.0025 - val_loss: 9.0139e-06 - val_root_mean_squared_error: 0.0030\n",
      "Epoch 19/60\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 6.2215e-06 - root_mean_squared_error: 0.0025 - val_loss: 8.9833e-06 - val_root_mean_squared_error: 0.0030\n",
      "Epoch 20/60\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 5.9024e-06 - root_mean_squared_error: 0.0024 - val_loss: 8.3752e-06 - val_root_mean_squared_error: 0.0029\n",
      "Epoch 21/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 5.8253e-06 - root_mean_squared_error: 0.0024 - val_loss: 8.2497e-06 - val_root_mean_squared_error: 0.0029\n",
      "Epoch 22/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 5.6112e-06 - root_mean_squared_error: 0.0024 - val_loss: 7.8293e-06 - val_root_mean_squared_error: 0.0028\n",
      "Epoch 23/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 5.5347e-06 - root_mean_squared_error: 0.0024 - val_loss: 7.6604e-06 - val_root_mean_squared_error: 0.0028\n",
      "Epoch 24/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 5.3866e-06 - root_mean_squared_error: 0.0023 - val_loss: 7.3604e-06 - val_root_mean_squared_error: 0.0027\n",
      "Epoch 25/60\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 5.3148e-06 - root_mean_squared_error: 0.0023 - val_loss: 7.1863e-06 - val_root_mean_squared_error: 0.0027\n",
      "Epoch 26/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 5.2069e-06 - root_mean_squared_error: 0.0023 - val_loss: 6.9717e-06 - val_root_mean_squared_error: 0.0026\n",
      "Epoch 27/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 5.1382e-06 - root_mean_squared_error: 0.0023 - val_loss: 6.8100e-06 - val_root_mean_squared_error: 0.0026\n",
      "Epoch 28/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 5.0543e-06 - root_mean_squared_error: 0.0022 - val_loss: 6.6547e-06 - val_root_mean_squared_error: 0.0026\n",
      "Epoch 29/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 4.9862e-06 - root_mean_squared_error: 0.0022 - val_loss: 6.5110e-06 - val_root_mean_squared_error: 0.0026\n",
      "Epoch 30/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 4.9160e-06 - root_mean_squared_error: 0.0022 - val_loss: 6.3904e-06 - val_root_mean_squared_error: 0.0025\n",
      "Epoch 31/60\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 4.8481e-06 - root_mean_squared_error: 0.0022 - val_loss: 6.2645e-06 - val_root_mean_squared_error: 0.0025\n",
      "Epoch 32/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 4.7847e-06 - root_mean_squared_error: 0.0022 - val_loss: 6.1594e-06 - val_root_mean_squared_error: 0.0025\n",
      "Epoch 33/60\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 4.7182e-06 - root_mean_squared_error: 0.0022 - val_loss: 6.0488e-06 - val_root_mean_squared_error: 0.0025\n",
      "Epoch 34/60\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 4.6567e-06 - root_mean_squared_error: 0.0022 - val_loss: 5.9486e-06 - val_root_mean_squared_error: 0.0024\n",
      "Epoch 35/60\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 4.5928e-06 - root_mean_squared_error: 0.0021 - val_loss: 5.8488e-06 - val_root_mean_squared_error: 0.0024\n",
      "Epoch 36/60\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 4.5309e-06 - root_mean_squared_error: 0.0021 - val_loss: 5.7508e-06 - val_root_mean_squared_error: 0.0024\n",
      "Epoch 37/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 4.4691e-06 - root_mean_squared_error: 0.0021 - val_loss: 5.6562e-06 - val_root_mean_squared_error: 0.0024\n",
      "Epoch 38/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 4.4069e-06 - root_mean_squared_error: 0.0021 - val_loss: 5.5607e-06 - val_root_mean_squared_error: 0.0024\n",
      "Epoch 39/60\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 4.3457e-06 - root_mean_squared_error: 0.0021 - val_loss: 5.4671e-06 - val_root_mean_squared_error: 0.0023\n",
      "Epoch 40/60\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 4.2839e-06 - root_mean_squared_error: 0.0021 - val_loss: 5.3736e-06 - val_root_mean_squared_error: 0.0023\n",
      "Epoch 41/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 4.2224e-06 - root_mean_squared_error: 0.0021 - val_loss: 5.2801e-06 - val_root_mean_squared_error: 0.0023\n",
      "Epoch 42/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 4.1609e-06 - root_mean_squared_error: 0.0020 - val_loss: 5.1872e-06 - val_root_mean_squared_error: 0.0023\n",
      "Epoch 43/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 4.0991e-06 - root_mean_squared_error: 0.0020 - val_loss: 5.0942e-06 - val_root_mean_squared_error: 0.0023\n",
      "Epoch 44/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 4.0374e-06 - root_mean_squared_error: 0.0020 - val_loss: 5.0013e-06 - val_root_mean_squared_error: 0.0022\n",
      "Epoch 45/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 3.9756e-06 - root_mean_squared_error: 0.0020 - val_loss: 4.9086e-06 - val_root_mean_squared_error: 0.0022\n",
      "Epoch 46/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 3.9136e-06 - root_mean_squared_error: 0.0020 - val_loss: 4.8158e-06 - val_root_mean_squared_error: 0.0022\n",
      "Epoch 47/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 85ms/step - loss: 3.8515e-06 - root_mean_squared_error: 0.0020 - val_loss: 4.7232e-06 - val_root_mean_squared_error: 0.0022\n",
      "Epoch 48/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 3.7894e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.6308e-06 - val_root_mean_squared_error: 0.0022\n",
      "Epoch 49/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 3.7271e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.5384e-06 - val_root_mean_squared_error: 0.0021\n",
      "Epoch 50/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 3.6649e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.4461e-06 - val_root_mean_squared_error: 0.0021\n",
      "Epoch 51/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 3.6026e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.3541e-06 - val_root_mean_squared_error: 0.0021\n",
      "Epoch 52/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 3.5403e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.2622e-06 - val_root_mean_squared_error: 0.0021\n",
      "Epoch 53/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 3.4781e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.1706e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 54/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 3.4161e-06 - root_mean_squared_error: 0.0018 - val_loss: 4.0793e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 55/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 3.3543e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.9883e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 56/60\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 3.2927e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.8977e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 57/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 3.2315e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.8075e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 58/60\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 3.1707e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.7179e-06 - val_root_mean_squared_error: 0.0019\n",
      "Epoch 59/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 3.1104e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.6288e-06 - val_root_mean_squared_error: 0.0019\n",
      "Epoch 60/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 3.0506e-06 - root_mean_squared_error: 0.0017 - val_loss: 3.5404e-06 - val_root_mean_squared_error: 0.0019\n"
     ]
    }
   ],
   "source": [
    "hist = lstm.fit(X_train, y_train,batch_size=700,epochs=60,verbose=1,validation_split=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the RSME for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['root_mean_squared_error'])\n",
    "plt.plot(hist.history['val_root_mean_squared_error'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the predictions made by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1.07,    Actual Value: 1.15\n",
      "Prediction: 1.19,    Actual Value: 1.15\n",
      "Prediction: 1.23,    Actual Value: 1.23\n",
      "Prediction: 1.27,    Actual Value: 1.27\n",
      "Prediction: 1.30,    Actual Value: 1.30\n",
      "Prediction: 1.38,    Actual Value: 1.34\n",
      "Prediction: 1.45,    Actual Value: 1.34\n",
      "Prediction: 1.37,    Actual Value: 1.35\n",
      "Prediction: 1.34,    Actual Value: 1.46\n",
      "Prediction: 1.30,    Actual Value: 1.42\n",
      "Prediction: 1.27,    Actual Value: 1.10\n",
      "Prediction: 1.28,    Actual Value: 1.24\n",
      "Prediction: 1.35,    Actual Value: 1.18\n",
      "Prediction: 1.44,    Actual Value: 1.32\n",
      "Prediction: 1.61,    Actual Value: 1.53\n",
      "Prediction: 1.74,    Actual Value: 1.90\n",
      "Prediction: 1.71,    Actual Value: 1.90\n",
      "Prediction: 1.73,    Actual Value: 1.94\n",
      "Prediction: 1.66,    Actual Value: 1.89\n",
      "Prediction: 1.65,    Actual Value: 1.92\n",
      "Prediction: 1.61,    Actual Value: 1.92\n",
      "Prediction: 1.51,    Actual Value: 1.80\n",
      "Prediction: 1.36,    Actual Value: 1.76\n",
      "Prediction: 1.19,    Actual Value: 1.37\n",
      "Prediction: 1.19,    Actual Value: 1.36\n",
      "Prediction: 1.10,    Actual Value: 0.94\n",
      "Prediction: 1.09,    Actual Value: 0.95\n",
      "Prediction: 0.96,    Actual Value: 0.98\n",
      "Prediction: 1.00,    Actual Value: 0.83\n",
      "Prediction: 1.03,    Actual Value: 0.72\n",
      "Prediction: 1.08,    Actual Value: 0.73\n",
      "Prediction: 1.01,    Actual Value: 0.72\n",
      "Prediction: 0.98,    Actual Value: 0.79\n",
      "Prediction: 1.05,    Actual Value: 0.68\n",
      "Prediction: 1.08,    Actual Value: 0.89\n",
      "Prediction: 1.13,    Actual Value: 1.04\n",
      "Prediction: 1.15,    Actual Value: 1.01\n",
      "Prediction: 1.22,    Actual Value: 1.02\n",
      "Prediction: 1.34,    Actual Value: 1.24\n",
      "Prediction: 1.48,    Actual Value: 1.60\n",
      "Prediction: 1.50,    Actual Value: 1.60\n",
      "Prediction: 1.51,    Actual Value: 1.63\n",
      "Prediction: 1.54,    Actual Value: 1.56\n",
      "Prediction: 1.55,    Actual Value: 1.56\n",
      "Prediction: 1.51,    Actual Value: 1.36\n",
      "Prediction: 1.53,    Actual Value: 1.61\n",
      "Prediction: 1.50,    Actual Value: 1.65\n",
      "Prediction: 1.50,    Actual Value: 1.68\n",
      "Prediction: 1.57,    Actual Value: 1.66\n",
      "Prediction: 1.59,    Actual Value: 1.36\n",
      "Prediction: 1.52,    Actual Value: 1.39\n",
      "Prediction: 1.54,    Actual Value: 1.38\n",
      "Prediction: 1.60,    Actual Value: 1.61\n",
      "Prediction: 1.67,    Actual Value: 1.78\n",
      "Prediction: 1.60,    Actual Value: 1.78\n",
      "Prediction: 1.50,    Actual Value: 1.54\n",
      "Prediction: 1.43,    Actual Value: 1.41\n",
      "Prediction: 1.36,    Actual Value: 1.29\n",
      "Prediction: 1.35,    Actual Value: 1.41\n",
      "Prediction: 1.37,    Actual Value: 1.41\n",
      "Prediction: 1.38,    Actual Value: 1.34\n",
      "Prediction: 1.29,    Actual Value: 1.34\n",
      "Prediction: 1.33,    Actual Value: 1.15\n",
      "Prediction: 1.31,    Actual Value: 1.04\n",
      "Prediction: 1.29,    Actual Value: 1.05\n",
      "Prediction: 1.23,    Actual Value: 1.07\n",
      "Prediction: 1.21,    Actual Value: 1.21\n",
      "Prediction: 1.21,    Actual Value: 1.21\n",
      "Prediction: 1.19,    Actual Value: 1.21\n",
      "Prediction: 1.15,    Actual Value: 1.21\n",
      "Prediction: 1.09,    Actual Value: 1.26\n",
      "Prediction: 1.11,    Actual Value: 1.25\n",
      "Prediction: 1.10,    Actual Value: 1.33\n",
      "Prediction: 1.09,    Actual Value: 1.26\n",
      "Prediction: 1.16,    Actual Value: 1.25\n",
      "Prediction: 1.03,    Actual Value: 1.29\n",
      "Prediction: 1.04,    Actual Value: 1.09\n",
      "Prediction: 0.95,    Actual Value: 1.11\n",
      "Prediction: 0.98,    Actual Value: 0.93\n",
      "Prediction: 1.04,    Actual Value: 0.73\n",
      "Prediction: 0.97,    Actual Value: 0.71\n",
      "Prediction: 0.96,    Actual Value: 0.66\n",
      "Prediction: 1.09,    Actual Value: 0.81\n",
      "Prediction: 0.91,    Actual Value: 0.82\n",
      "Prediction: 0.92,    Actual Value: 0.82\n",
      "Prediction: 0.84,    Actual Value: 0.67\n",
      "Prediction: 0.91,    Actual Value: 0.71\n",
      "Prediction: 0.93,    Actual Value: 0.72\n",
      "Prediction: 0.85,    Actual Value: 0.72\n",
      "Prediction: 0.78,    Actual Value: 0.80\n",
      "Prediction: 0.81,    Actual Value: 0.84\n",
      "Prediction: 0.82,    Actual Value: 0.81\n",
      "Prediction: 0.80,    Actual Value: 0.72\n",
      "Prediction: 0.87,    Actual Value: 0.71\n",
      "Prediction: 0.81,    Actual Value: 0.78\n",
      "Prediction: 0.72,    Actual Value: 0.70\n",
      "Prediction: 0.65,    Actual Value: 0.70\n",
      "Prediction: 0.67,    Actual Value: 0.69\n",
      "Prediction: 0.61,    Actual Value: 0.72\n",
      "Prediction: 0.56,    Actual Value: 0.67\n",
      "Prediction: 0.59,    Actual Value: 0.62\n",
      "Prediction: 0.56,    Actual Value: 0.62\n",
      "Prediction: 0.68,    Actual Value: 0.57\n",
      "Prediction: 0.57,    Actual Value: 0.69\n",
      "Prediction: 0.79,    Actual Value: 0.68\n",
      "Prediction: 0.72,    Actual Value: 0.67\n",
      "Prediction: 0.65,    Actual Value: 0.80\n",
      "Prediction: 0.68,    Actual Value: 0.74\n",
      "Prediction: 0.89,    Actual Value: 0.76\n",
      "Prediction: 0.93,    Actual Value: 0.87\n",
      "Prediction: 0.92,    Actual Value: 0.89\n",
      "Prediction: 1.01,    Actual Value: 0.88\n",
      "Prediction: 0.97,    Actual Value: 0.88\n",
      "Prediction: 1.05,    Actual Value: 0.93\n",
      "Prediction: 1.20,    Actual Value: 1.49\n",
      "Prediction: 1.19,    Actual Value: 1.52\n",
      "Prediction: 1.21,    Actual Value: 1.36\n",
      "Prediction: 1.14,    Actual Value: 1.41\n",
      "Prediction: 1.13,    Actual Value: 1.31\n",
      "Prediction: 1.16,    Actual Value: 1.33\n",
      "Prediction: 1.18,    Actual Value: 1.32\n",
      "Prediction: 1.12,    Actual Value: 1.24\n",
      "Prediction: 1.15,    Actual Value: 1.26\n",
      "Prediction: 1.15,    Actual Value: 1.28\n",
      "Prediction: 1.07,    Actual Value: 0.79\n",
      "Prediction: 1.06,    Actual Value: 0.76\n",
      "Prediction: 1.06,    Actual Value: 0.81\n",
      "Prediction: 1.04,    Actual Value: 0.66\n",
      "Prediction: 1.06,    Actual Value: 0.77\n",
      "Prediction: 1.04,    Actual Value: 0.83\n",
      "Prediction: 1.06,    Actual Value: 0.83\n",
      "Prediction: 1.22,    Actual Value: 0.97\n",
      "Prediction: 1.17,    Actual Value: 1.09\n",
      "Prediction: 1.14,    Actual Value: 1.06\n",
      "Prediction: 1.16,    Actual Value: 1.25\n",
      "Prediction: 1.21,    Actual Value: 1.23\n",
      "Prediction: 1.17,    Actual Value: 1.21\n",
      "Prediction: 1.20,    Actual Value: 1.33\n",
      "Prediction: 1.12,    Actual Value: 1.22\n",
      "Prediction: 1.12,    Actual Value: 1.20\n",
      "Prediction: 1.11,    Actual Value: 1.18\n",
      "Prediction: 1.03,    Actual Value: 0.94\n",
      "Prediction: 1.04,    Actual Value: 1.01\n",
      "Prediction: 1.10,    Actual Value: 0.96\n",
      "Prediction: 1.01,    Actual Value: 0.90\n",
      "Prediction: 1.04,    Actual Value: 0.81\n",
      "Prediction: 1.03,    Actual Value: 1.02\n",
      "Prediction: 1.02,    Actual Value: 0.92\n",
      "Prediction: 0.96,    Actual Value: 0.92\n",
      "Prediction: 0.95,    Actual Value: 0.93\n",
      "Prediction: 0.95,    Actual Value: 0.96\n",
      "Prediction: 0.95,    Actual Value: 0.90\n",
      "Prediction: 0.96,    Actual Value: 0.88\n",
      "Prediction: 0.95,    Actual Value: 0.89\n",
      "Prediction: 0.97,    Actual Value: 0.85\n",
      "Prediction: 0.87,    Actual Value: 0.86\n",
      "Prediction: 0.91,    Actual Value: 0.68\n",
      "Prediction: 0.88,    Actual Value: 0.71\n",
      "Prediction: 0.92,    Actual Value: 0.71\n",
      "Prediction: 0.85,    Actual Value: 0.73\n",
      "Prediction: 0.91,    Actual Value: 0.76\n",
      "Prediction: 0.92,    Actual Value: 0.91\n",
      "Prediction: 0.97,    Actual Value: 0.83\n",
      "Prediction: 0.89,    Actual Value: 0.83\n",
      "Prediction: 0.92,    Actual Value: 0.90\n",
      "Prediction: 0.96,    Actual Value: 1.13\n",
      "Prediction: 1.00,    Actual Value: 0.98\n",
      "Prediction: 0.97,    Actual Value: 0.98\n",
      "Prediction: 0.97,    Actual Value: 1.07\n",
      "Prediction: 1.01,    Actual Value: 1.09\n",
      "Prediction: 1.02,    Actual Value: 1.20\n",
      "Prediction: 1.06,    Actual Value: 1.05\n",
      "Prediction: 1.05,    Actual Value: 1.12\n",
      "Prediction: 1.03,    Actual Value: 1.14\n",
      "Prediction: 1.01,    Actual Value: 1.06\n",
      "Prediction: 1.00,    Actual Value: 0.97\n",
      "Prediction: 1.01,    Actual Value: 0.97\n",
      "Prediction: 0.95,    Actual Value: 1.00\n",
      "Prediction: 0.90,    Actual Value: 0.99\n",
      "Prediction: 0.90,    Actual Value: 1.02\n",
      "Prediction: 0.85,    Actual Value: 0.89\n",
      "Prediction: 0.90,    Actual Value: 0.88\n",
      "Prediction: 0.84,    Actual Value: 0.88\n",
      "Prediction: 0.84,    Actual Value: 0.85\n",
      "Prediction: 0.88,    Actual Value: 0.95\n",
      "Prediction: 0.91,    Actual Value: 0.74\n",
      "Prediction: 0.89,    Actual Value: 0.84\n",
      "Prediction: 0.82,    Actual Value: 0.84\n",
      "Prediction: 0.81,    Actual Value: 0.79\n",
      "Prediction: 0.79,    Actual Value: 0.74\n",
      "Prediction: 0.77,    Actual Value: 0.77\n",
      "Prediction: 0.81,    Actual Value: 0.77\n",
      "Prediction: 0.82,    Actual Value: 0.82\n",
      "Prediction: 0.79,    Actual Value: 1.05\n",
      "Prediction: 0.81,    Actual Value: 0.95\n",
      "Prediction: 0.83,    Actual Value: 1.04\n",
      "Prediction: 0.90,    Actual Value: 0.93\n",
      "Prediction: 0.89,    Actual Value: 1.01\n",
      "Prediction: 0.93,    Actual Value: 1.01\n",
      "Prediction: 0.93,    Actual Value: 1.01\n",
      "Prediction: 0.95,    Actual Value: 1.04\n",
      "Prediction: 0.95,    Actual Value: 1.04\n",
      "Prediction: 0.93,    Actual Value: 0.93\n",
      "Prediction: 0.88,    Actual Value: 0.73\n",
      "Prediction: 0.86,    Actual Value: 0.82\n",
      "Prediction: 0.78,    Actual Value: 0.62\n",
      "Prediction: 0.77,    Actual Value: 0.52\n",
      "Prediction: 0.74,    Actual Value: 0.50\n",
      "Prediction: 0.65,    Actual Value: 0.57\n",
      "Prediction: 0.70,    Actual Value: 0.58\n",
      "Prediction: 0.68,    Actual Value: 0.65\n",
      "Prediction: 0.70,    Actual Value: 0.68\n",
      "Prediction: 0.67,    Actual Value: 0.78\n",
      "Prediction: 0.72,    Actual Value: 0.76\n",
      "Prediction: 0.76,    Actual Value: 0.69\n",
      "Prediction: 0.74,    Actual Value: 0.77\n",
      "Prediction: 0.74,    Actual Value: 0.66\n",
      "Prediction: 0.79,    Actual Value: 0.75\n",
      "Prediction: 0.78,    Actual Value: 0.75\n",
      "Prediction: 0.75,    Actual Value: 0.76\n",
      "Prediction: 0.75,    Actual Value: 0.78\n",
      "Prediction: 0.72,    Actual Value: 0.78\n",
      "Prediction: 0.72,    Actual Value: 0.75\n",
      "Prediction: 0.69,    Actual Value: 0.94\n",
      "Prediction: 0.75,    Actual Value: 0.90\n",
      "Prediction: 0.80,    Actual Value: 0.85\n",
      "Prediction: 0.87,    Actual Value: 0.89\n",
      "Prediction: 0.87,    Actual Value: 0.79\n",
      "Prediction: 0.95,    Actual Value: 1.02\n",
      "Prediction: 0.92,    Actual Value: 1.01\n",
      "Prediction: 0.96,    Actual Value: 1.03\n",
      "Prediction: 0.92,    Actual Value: 1.03\n",
      "Prediction: 0.94,    Actual Value: 1.07\n",
      "Prediction: 0.94,    Actual Value: 0.88\n",
      "Prediction: 0.91,    Actual Value: 0.88\n",
      "Prediction: 0.83,    Actual Value: 0.89\n",
      "Prediction: 0.84,    Actual Value: 0.89\n",
      "Prediction: 0.79,    Actual Value: 0.82\n",
      "Prediction: 0.70,    Actual Value: 0.64\n",
      "Prediction: 0.66,    Actual Value: 0.64\n",
      "Prediction: 0.69,    Actual Value: 0.55\n",
      "Prediction: 0.69,    Actual Value: 0.59\n",
      "Prediction: 0.60,    Actual Value: 0.55\n",
      "Prediction: 0.60,    Actual Value: 0.61\n",
      "Prediction: 0.62,    Actual Value: 0.62\n",
      "Prediction: 0.61,    Actual Value: 0.64\n",
      "Prediction: 0.63,    Actual Value: 0.58\n",
      "Prediction: 0.64,    Actual Value: 0.58\n",
      "Prediction: 0.63,    Actual Value: 0.64\n",
      "Prediction: 0.63,    Actual Value: 0.63\n",
      "Prediction: 0.61,    Actual Value: 0.55\n",
      "Prediction: 0.57,    Actual Value: 0.54\n",
      "Prediction: 0.57,    Actual Value: 0.56\n",
      "Prediction: 0.57,    Actual Value: 0.60\n",
      "Prediction: 0.61,    Actual Value: 0.58\n",
      "Prediction: 0.64,    Actual Value: 0.60\n",
      "Prediction: 0.63,    Actual Value: 0.65\n",
      "Prediction: 0.70,    Actual Value: 0.64\n",
      "Prediction: 0.77,    Actual Value: 0.58\n",
      "Prediction: 0.72,    Actual Value: 0.58\n",
      "Prediction: 0.78,    Actual Value: 0.58\n",
      "Prediction: 0.80,    Actual Value: 0.66\n",
      "Prediction: 0.92,    Actual Value: 0.99\n",
      "Prediction: 1.01,    Actual Value: 0.89\n",
      "Prediction: 1.12,    Actual Value: 1.35\n",
      "Prediction: 1.06,    Actual Value: 1.34\n",
      "Prediction: 1.12,    Actual Value: 1.34\n",
      "Prediction: 1.08,    Actual Value: 1.34\n",
      "Prediction: 1.08,    Actual Value: 1.34\n",
      "Prediction: 1.09,    Actual Value: 1.37\n",
      "Prediction: 1.03,    Actual Value: 1.37\n",
      "Prediction: 0.85,    Actual Value: 1.32\n",
      "Prediction: 0.73,    Actual Value: 1.03\n",
      "Prediction: 0.69,    Actual Value: 1.06\n",
      "Prediction: 0.69,    Actual Value: 0.53\n",
      "Prediction: 0.68,    Actual Value: 0.52\n",
      "Prediction: 0.63,    Actual Value: 0.45\n",
      "Prediction: 0.62,    Actual Value: 0.53\n",
      "Prediction: 0.61,    Actual Value: 0.54\n",
      "Prediction: 0.60,    Actual Value: 0.47\n",
      "Prediction: 0.62,    Actual Value: 0.52\n",
      "Prediction: 0.69,    Actual Value: 0.59\n",
      "Prediction: 0.59,    Actual Value: 0.56\n",
      "Prediction: 0.58,    Actual Value: 0.56\n",
      "Prediction: 0.56,    Actual Value: 0.57\n",
      "Prediction: 0.56,    Actual Value: 0.57\n",
      "Prediction: 0.56,    Actual Value: 0.59\n",
      "Prediction: 0.53,    Actual Value: 0.50\n",
      "Prediction: 0.54,    Actual Value: 0.50\n",
      "Prediction: 0.51,    Actual Value: 0.47\n",
      "Prediction: 0.49,    Actual Value: 0.34\n",
      "Prediction: 0.45,    Actual Value: 0.33\n",
      "Prediction: 0.51,    Actual Value: 0.34\n",
      "Prediction: 0.56,    Actual Value: 0.34\n",
      "Prediction: 0.52,    Actual Value: 0.46\n",
      "Prediction: 0.46,    Actual Value: 0.51\n",
      "Prediction: 0.52,    Actual Value: 0.51\n",
      "Prediction: 0.54,    Actual Value: 0.49\n",
      "Prediction: 0.58,    Actual Value: 0.56\n",
      "Prediction: 0.60,    Actual Value: 0.68\n",
      "Prediction: 0.59,    Actual Value: 0.70\n",
      "Prediction: 0.63,    Actual Value: 0.81\n",
      "Prediction: 0.69,    Actual Value: 0.89\n",
      "Prediction: 0.75,    Actual Value: 0.89\n",
      "Prediction: 0.73,    Actual Value: 0.82\n",
      "Prediction: 0.66,    Actual Value: 0.74\n",
      "Prediction: 0.63,    Actual Value: 0.76\n",
      "Prediction: 0.59,    Actual Value: 0.80\n",
      "Prediction: 0.63,    Actual Value: 0.69\n",
      "Prediction: 0.63,    Actual Value: 0.71\n",
      "Prediction: 0.68,    Actual Value: 0.72\n",
      "Prediction: 0.71,    Actual Value: 0.73\n",
      "Prediction: 0.68,    Actual Value: 0.65\n",
      "Prediction: 0.71,    Actual Value: 0.64\n",
      "Prediction: 0.69,    Actual Value: 0.64\n",
      "Prediction: 0.70,    Actual Value: 0.64\n",
      "Prediction: 0.72,    Actual Value: 0.61\n",
      "Prediction: 0.68,    Actual Value: 0.60\n",
      "Prediction: 0.66,    Actual Value: 0.61\n",
      "Prediction: 0.68,    Actual Value: 0.60\n",
      "Prediction: 0.69,    Actual Value: 0.59\n",
      "Prediction: 0.67,    Actual Value: 0.44\n",
      "Prediction: 0.66,    Actual Value: 0.50\n",
      "Prediction: 0.71,    Actual Value: 0.49\n",
      "Prediction: 0.67,    Actual Value: 0.49\n",
      "Prediction: 0.73,    Actual Value: 1.31\n",
      "Prediction: 0.86,    Actual Value: 1.35\n",
      "Prediction: 0.98,    Actual Value: 1.42\n",
      "Prediction: 1.04,    Actual Value: 1.40\n",
      "Prediction: 1.07,    Actual Value: 1.36\n",
      "Prediction: 1.11,    Actual Value: 1.37\n",
      "Prediction: 1.13,    Actual Value: 1.36\n",
      "Prediction: 1.19,    Actual Value: 1.34\n",
      "Prediction: 1.10,    Actual Value: 1.32\n",
      "Prediction: 0.99,    Actual Value: 1.31\n",
      "Prediction: 0.95,    Actual Value: 0.91\n",
      "Prediction: 0.90,    Actual Value: 0.88\n",
      "Prediction: 0.82,    Actual Value: 0.73\n",
      "Prediction: 0.83,    Actual Value: 0.76\n",
      "Prediction: 0.87,    Actual Value: 0.77\n",
      "Prediction: 0.79,    Actual Value: 0.69\n",
      "Prediction: 0.78,    Actual Value: 0.68\n",
      "Prediction: 0.74,    Actual Value: 0.65\n",
      "Prediction: 0.79,    Actual Value: 0.71\n",
      "Prediction: 0.87,    Actual Value: 0.71\n",
      "Prediction: 0.82,    Actual Value: 0.51\n",
      "Prediction: 0.81,    Actual Value: 0.54\n",
      "Prediction: 0.74,    Actual Value: 0.53\n",
      "Prediction: 0.79,    Actual Value: 0.47\n",
      "Prediction: 0.89,    Actual Value: 0.57\n",
      "Prediction: 0.93,    Actual Value: 0.90\n",
      "Prediction: 0.86,    Actual Value: 0.87\n",
      "Prediction: 0.82,    Actual Value: 0.88\n",
      "Prediction: 0.86,    Actual Value: 0.88\n",
      "Prediction: 0.94,    Actual Value: 1.00\n",
      "Prediction: 0.96,    Actual Value: 0.97\n",
      "Prediction: 0.94,    Actual Value: 0.91\n",
      "Prediction: 0.88,    Actual Value: 0.88\n",
      "Prediction: 0.81,    Actual Value: 0.90\n",
      "Prediction: 0.71,    Actual Value: 0.94\n",
      "Prediction: 0.72,    Actual Value: 0.73\n",
      "Prediction: 0.75,    Actual Value: 0.76\n",
      "Prediction: 0.72,    Actual Value: 0.75\n",
      "Prediction: 0.75,    Actual Value: 0.75\n",
      "Prediction: 0.72,    Actual Value: 0.58\n",
      "Prediction: 0.75,    Actual Value: 0.57\n",
      "Prediction: 0.74,    Actual Value: 0.61\n",
      "Prediction: 0.77,    Actual Value: 0.59\n",
      "Prediction: 0.69,    Actual Value: 0.61\n",
      "Prediction: 0.73,    Actual Value: 0.67\n",
      "Prediction: 0.77,    Actual Value: 0.81\n",
      "Prediction: 0.74,    Actual Value: 0.84\n",
      "Prediction: 0.74,    Actual Value: 0.82\n",
      "Prediction: 0.75,    Actual Value: 0.82\n",
      "Prediction: 0.77,    Actual Value: 0.65\n",
      "Prediction: 0.74,    Actual Value: 0.71\n",
      "Prediction: 0.72,    Actual Value: 0.72\n",
      "Prediction: 0.65,    Actual Value: 0.73\n",
      "Prediction: 0.70,    Actual Value: 0.73\n",
      "Prediction: 0.68,    Actual Value: 0.77\n",
      "Prediction: 0.71,    Actual Value: 0.62\n",
      "Prediction: 0.69,    Actual Value: 0.58\n",
      "Prediction: 0.73,    Actual Value: 0.61\n",
      "Prediction: 0.70,    Actual Value: 0.65\n",
      "Prediction: 0.70,    Actual Value: 0.65\n",
      "Prediction: 0.69,    Actual Value: 0.57\n",
      "Prediction: 0.69,    Actual Value: 0.65\n",
      "Prediction: 0.70,    Actual Value: 0.61\n",
      "Prediction: 0.73,    Actual Value: 0.74\n",
      "Prediction: 0.76,    Actual Value: 0.64\n",
      "Prediction: 0.76,    Actual Value: 0.80\n",
      "Prediction: 0.72,    Actual Value: 0.82\n",
      "Prediction: 0.67,    Actual Value: 0.80\n",
      "Prediction: 0.73,    Actual Value: 0.78\n",
      "Prediction: 0.79,    Actual Value: 0.87\n",
      "Prediction: 0.73,    Actual Value: 0.84\n",
      "Prediction: 0.68,    Actual Value: 0.80\n",
      "Prediction: 0.66,    Actual Value: 0.79\n",
      "Prediction: 0.62,    Actual Value: 0.66\n",
      "Prediction: 0.66,    Actual Value: 0.73\n",
      "Prediction: 0.62,    Actual Value: 0.59\n",
      "Prediction: 0.58,    Actual Value: 0.58\n",
      "Prediction: 0.63,    Actual Value: 0.58\n",
      "Prediction: 0.59,    Actual Value: 0.60\n",
      "Prediction: 0.58,    Actual Value: 0.43\n",
      "Prediction: 0.54,    Actual Value: 0.43\n",
      "Prediction: 0.57,    Actual Value: 0.49\n",
      "Prediction: 0.58,    Actual Value: 0.50\n",
      "Prediction: 0.52,    Actual Value: 0.58\n",
      "Prediction: 0.51,    Actual Value: 0.51\n",
      "Prediction: 0.49,    Actual Value: 0.45\n",
      "Prediction: 0.46,    Actual Value: 0.40\n",
      "Prediction: 0.40,    Actual Value: 0.43\n",
      "Prediction: 0.42,    Actual Value: 0.43\n",
      "Prediction: 0.43,    Actual Value: 0.37\n",
      "Prediction: 0.45,    Actual Value: 0.37\n",
      "Prediction: 0.44,    Actual Value: 0.50\n",
      "Prediction: 0.47,    Actual Value: 0.49\n",
      "Prediction: 0.54,    Actual Value: 0.40\n",
      "Prediction: 0.61,    Actual Value: 0.41\n",
      "Prediction: 0.60,    Actual Value: 0.47\n",
      "Prediction: 0.61,    Actual Value: 0.46\n",
      "Prediction: 0.68,    Actual Value: 0.60\n",
      "Prediction: 0.72,    Actual Value: 0.82\n",
      "Prediction: 0.78,    Actual Value: 0.84\n",
      "Prediction: 0.88,    Actual Value: 1.03\n",
      "Prediction: 0.91,    Actual Value: 1.03\n",
      "Prediction: 0.90,    Actual Value: 1.04\n",
      "Prediction: 0.98,    Actual Value: 1.13\n",
      "Prediction: 1.00,    Actual Value: 1.16\n",
      "Prediction: 0.98,    Actual Value: 1.09\n",
      "Prediction: 0.96,    Actual Value: 1.23\n",
      "Prediction: 0.96,    Actual Value: 1.20\n",
      "Prediction: 0.92,    Actual Value: 0.98\n",
      "Prediction: 0.85,    Actual Value: 0.89\n",
      "Prediction: 0.80,    Actual Value: 0.85\n",
      "Prediction: 0.75,    Actual Value: 0.82\n",
      "Prediction: 0.83,    Actual Value: 0.81\n",
      "Prediction: 0.78,    Actual Value: 0.67\n",
      "Prediction: 0.76,    Actual Value: 0.64\n",
      "Prediction: 0.71,    Actual Value: 0.66\n",
      "Prediction: 0.69,    Actual Value: 0.56\n",
      "Prediction: 0.66,    Actual Value: 0.56\n",
      "Prediction: 0.64,    Actual Value: 0.56\n",
      "Prediction: 0.58,    Actual Value: 0.56\n",
      "Prediction: 0.54,    Actual Value: 0.50\n",
      "Prediction: 0.55,    Actual Value: 0.50\n",
      "Prediction: 0.55,    Actual Value: 0.56\n",
      "Prediction: 0.51,    Actual Value: 0.53\n",
      "Prediction: 0.51,    Actual Value: 0.55\n",
      "Prediction: 0.54,    Actual Value: 0.57\n",
      "Prediction: 0.54,    Actual Value: 0.49\n",
      "Prediction: 0.54,    Actual Value: 0.58\n",
      "Prediction: 0.54,    Actual Value: 0.58\n",
      "Prediction: 0.53,    Actual Value: 0.62\n",
      "Prediction: 0.57,    Actual Value: 0.63\n",
      "Prediction: 0.58,    Actual Value: 0.61\n",
      "Prediction: 0.59,    Actual Value: 0.61\n",
      "Prediction: 0.60,    Actual Value: 0.72\n",
      "Prediction: 0.54,    Actual Value: 0.67\n",
      "Prediction: 0.52,    Actual Value: 0.62\n",
      "Prediction: 0.56,    Actual Value: 0.68\n",
      "Prediction: 0.58,    Actual Value: 0.62\n",
      "Prediction: 0.56,    Actual Value: 0.63\n",
      "Prediction: 0.55,    Actual Value: 0.59\n",
      "Prediction: 0.52,    Actual Value: 0.58\n",
      "Prediction: 0.49,    Actual Value: 0.60\n",
      "Prediction: 0.53,    Actual Value: 0.63\n",
      "Prediction: 0.56,    Actual Value: 0.51\n",
      "Prediction: 0.53,    Actual Value: 0.52\n",
      "Prediction: 0.52,    Actual Value: 0.48\n",
      "Prediction: 0.52,    Actual Value: 0.42\n",
      "Prediction: 0.54,    Actual Value: 0.58\n",
      "Prediction: 0.59,    Actual Value: 0.58\n",
      "Prediction: 0.63,    Actual Value: 0.65\n",
      "Prediction: 0.65,    Actual Value: 0.65\n",
      "Prediction: 0.63,    Actual Value: 0.63\n",
      "Prediction: 0.62,    Actual Value: 0.60\n",
      "Prediction: 0.65,    Actual Value: 0.57\n",
      "Prediction: 0.66,    Actual Value: 0.56\n",
      "Prediction: 0.67,    Actual Value: 0.74\n",
      "Prediction: 0.66,    Actual Value: 0.72\n",
      "Prediction: 0.64,    Actual Value: 0.59\n",
      "Prediction: 0.61,    Actual Value: 0.59\n",
      "Prediction: 0.63,    Actual Value: 0.57\n",
      "Prediction: 0.67,    Actual Value: 0.72\n",
      "Prediction: 0.67,    Actual Value: 0.72\n",
      "Prediction: 0.66,    Actual Value: 0.76\n",
      "Prediction: 0.60,    Actual Value: 0.75\n",
      "Prediction: 0.63,    Actual Value: 0.75\n",
      "Prediction: 0.65,    Actual Value: 0.67\n",
      "Prediction: 0.69,    Actual Value: 0.66\n",
      "Prediction: 0.65,    Actual Value: 0.58\n",
      "Prediction: 0.60,    Actual Value: 0.59\n",
      "Prediction: 0.57,    Actual Value: 0.60\n",
      "Prediction: 0.54,    Actual Value: 0.44\n",
      "Prediction: 0.55,    Actual Value: 0.51\n",
      "Prediction: 0.56,    Actual Value: 0.40\n",
      "Prediction: 0.53,    Actual Value: 0.40\n",
      "Prediction: 0.54,    Actual Value: 0.39\n",
      "Prediction: 0.58,    Actual Value: 0.59\n",
      "Prediction: 0.63,    Actual Value: 0.68\n",
      "Prediction: 0.58,    Actual Value: 0.69\n",
      "Prediction: 0.56,    Actual Value: 0.65\n",
      "Prediction: 0.56,    Actual Value: 0.61\n",
      "Prediction: 0.50,    Actual Value: 0.62\n",
      "Prediction: 0.51,    Actual Value: 0.60\n",
      "Prediction: 0.48,    Actual Value: 0.63\n",
      "Prediction: 0.51,    Actual Value: 0.65\n",
      "Prediction: 0.46,    Actual Value: 0.65\n",
      "Prediction: 0.47,    Actual Value: 0.48\n",
      "Prediction: 0.40,    Actual Value: 0.30\n",
      "Prediction: 0.43,    Actual Value: 0.24\n",
      "Prediction: 0.40,    Actual Value: 0.32\n",
      "Prediction: 0.40,    Actual Value: 0.32\n",
      "Prediction: 0.39,    Actual Value: 0.33\n",
      "Prediction: 0.43,    Actual Value: 0.37\n",
      "Prediction: 0.41,    Actual Value: 0.35\n",
      "Prediction: 0.41,    Actual Value: 0.42\n",
      "Prediction: 0.42,    Actual Value: 0.50\n",
      "Prediction: 0.45,    Actual Value: 0.51\n",
      "Prediction: 0.44,    Actual Value: 0.55\n",
      "Prediction: 0.46,    Actual Value: 0.55\n",
      "Prediction: 0.45,    Actual Value: 0.53\n",
      "Prediction: 0.51,    Actual Value: 0.52\n",
      "Prediction: 0.55,    Actual Value: 0.55\n",
      "Prediction: 0.53,    Actual Value: 0.53\n",
      "Prediction: 0.53,    Actual Value: 0.53\n",
      "Prediction: 0.41,    Actual Value: 0.46\n",
      "Prediction: 0.43,    Actual Value: 0.37\n",
      "Prediction: 0.44,    Actual Value: 0.38\n",
      "Prediction: 0.56,    Actual Value: 0.44\n",
      "Prediction: 0.52,    Actual Value: 0.45\n",
      "Prediction: 0.64,    Actual Value: 0.59\n",
      "Prediction: 0.55,    Actual Value: 0.76\n",
      "Prediction: 0.63,    Actual Value: 0.75\n",
      "Prediction: 0.60,    Actual Value: 0.74\n",
      "Prediction: 0.62,    Actual Value: 0.77\n",
      "Prediction: 0.60,    Actual Value: 0.77\n",
      "Prediction: 0.61,    Actual Value: 0.84\n",
      "Prediction: 0.62,    Actual Value: 0.83\n",
      "Prediction: 0.60,    Actual Value: 0.78\n",
      "Prediction: 0.63,    Actual Value: 0.84\n",
      "Prediction: 0.56,    Actual Value: 0.71\n",
      "Prediction: 0.58,    Actual Value: 0.60\n",
      "Prediction: 0.58,    Actual Value: 0.61\n",
      "Prediction: 0.59,    Actual Value: 0.62\n",
      "Prediction: 0.56,    Actual Value: 0.67\n",
      "Prediction: 0.76,    Actual Value: 0.68\n",
      "Prediction: 0.68,    Actual Value: 0.62\n",
      "Prediction: 0.73,    Actual Value: 0.62\n",
      "Prediction: 0.74,    Actual Value: 0.71\n",
      "Prediction: 0.75,    Actual Value: 0.75\n",
      "Prediction: 0.80,    Actual Value: 0.84\n",
      "Prediction: 0.84,    Actual Value: 0.87\n",
      "Prediction: 0.80,    Actual Value: 0.86\n",
      "Prediction: 0.82,    Actual Value: 0.86\n",
      "Prediction: 0.78,    Actual Value: 0.76\n",
      "Prediction: 0.71,    Actual Value: 0.75\n",
      "Prediction: 0.72,    Actual Value: 0.74\n",
      "Prediction: 0.65,    Actual Value: 0.67\n",
      "Prediction: 0.59,    Actual Value: 0.65\n",
      "Prediction: 0.55,    Actual Value: 0.45\n",
      "Prediction: 0.55,    Actual Value: 0.40\n",
      "Prediction: 0.56,    Actual Value: 0.40\n",
      "Prediction: 0.58,    Actual Value: 0.40\n",
      "Prediction: 0.54,    Actual Value: 0.39\n",
      "Prediction: 0.50,    Actual Value: 0.45\n",
      "Prediction: 0.49,    Actual Value: 0.48\n",
      "Prediction: 0.41,    Actual Value: 0.55\n",
      "Prediction: 0.44,    Actual Value: 0.61\n",
      "Prediction: 0.46,    Actual Value: 0.61\n",
      "Prediction: 0.50,    Actual Value: 0.61\n",
      "Prediction: 0.51,    Actual Value: 0.52\n",
      "Prediction: 0.49,    Actual Value: 0.52\n",
      "Prediction: 0.49,    Actual Value: 0.58\n",
      "Prediction: 0.52,    Actual Value: 0.58\n",
      "Prediction: 0.45,    Actual Value: 0.55\n",
      "Prediction: 0.51,    Actual Value: 0.47\n",
      "Prediction: 0.48,    Actual Value: 0.43\n",
      "Prediction: 0.45,    Actual Value: 0.44\n",
      "Prediction: 0.39,    Actual Value: 0.44\n",
      "Prediction: 0.34,    Actual Value: 0.46\n",
      "Prediction: 0.33,    Actual Value: 0.49\n",
      "Prediction: 0.39,    Actual Value: 0.51\n",
      "Prediction: 0.37,    Actual Value: 0.46\n",
      "Prediction: 0.33,    Actual Value: 0.46\n",
      "Prediction: 0.32,    Actual Value: 0.45\n",
      "Prediction: 0.32,    Actual Value: 0.42\n",
      "Prediction: 0.36,    Actual Value: 0.40\n",
      "Prediction: 0.37,    Actual Value: 0.34\n",
      "Prediction: 0.43,    Actual Value: 0.36\n",
      "Prediction: 0.39,    Actual Value: 0.36\n",
      "Prediction: 0.38,    Actual Value: 0.31\n",
      "Prediction: 0.33,    Actual Value: 0.34\n",
      "Prediction: 0.48,    Actual Value: 0.36\n",
      "Prediction: 0.44,    Actual Value: 0.37\n",
      "Prediction: 0.45,    Actual Value: 0.37\n",
      "Prediction: 0.40,    Actual Value: 0.37\n",
      "Prediction: 0.37,    Actual Value: 0.37\n",
      "Prediction: 0.38,    Actual Value: 0.46\n",
      "Prediction: 0.40,    Actual Value: 0.59\n",
      "Prediction: 0.44,    Actual Value: 0.57\n",
      "Prediction: 0.43,    Actual Value: 0.56\n",
      "Prediction: 0.43,    Actual Value: 0.56\n",
      "Prediction: 0.28,    Actual Value: 0.55\n",
      "Prediction: 0.37,    Actual Value: 0.55\n",
      "Prediction: 0.42,    Actual Value: 0.55\n",
      "Prediction: 0.45,    Actual Value: 0.55\n",
      "Prediction: 0.40,    Actual Value: 0.60\n",
      "Prediction: 0.42,    Actual Value: 0.53\n",
      "Prediction: 0.31,    Actual Value: 0.25\n",
      "Prediction: 0.36,    Actual Value: 0.26\n",
      "Prediction: 0.25,    Actual Value: 0.35\n",
      "Prediction: 0.29,    Actual Value: 0.35\n",
      "Prediction: 0.43,    Actual Value: 0.39\n",
      "Prediction: 0.42,    Actual Value: 0.40\n",
      "Prediction: 0.43,    Actual Value: 0.45\n",
      "Prediction: 0.47,    Actual Value: 0.51\n",
      "Prediction: 0.50,    Actual Value: 0.46\n",
      "Prediction: 0.42,    Actual Value: 0.46\n",
      "Prediction: 0.49,    Actual Value: 0.47\n",
      "Prediction: 0.44,    Actual Value: 0.49\n",
      "Prediction: 0.50,    Actual Value: 0.41\n",
      "Prediction: 0.47,    Actual Value: 0.37\n",
      "Prediction: 0.48,    Actual Value: 0.43\n",
      "Prediction: 0.47,    Actual Value: 0.43\n",
      "Prediction: 0.48,    Actual Value: 0.46\n",
      "Prediction: 0.51,    Actual Value: 0.42\n",
      "Prediction: 0.46,    Actual Value: 0.42\n",
      "Prediction: 0.53,    Actual Value: 0.48\n",
      "Prediction: 0.46,    Actual Value: 0.45\n",
      "Prediction: 0.51,    Actual Value: 0.48\n",
      "Prediction: 0.49,    Actual Value: 0.48\n",
      "Prediction: 0.53,    Actual Value: 0.50\n",
      "Prediction: 0.48,    Actual Value: 0.48\n",
      "Prediction: 0.51,    Actual Value: 0.53\n",
      "Prediction: 0.51,    Actual Value: 0.49\n",
      "Prediction: 0.52,    Actual Value: 0.50\n",
      "Prediction: 0.57,    Actual Value: 0.57\n",
      "Prediction: 0.67,    Actual Value: 0.94\n",
      "Prediction: 0.74,    Actual Value: 0.93\n",
      "Prediction: 0.82,    Actual Value: 0.82\n",
      "Prediction: 0.77,    Actual Value: 0.82\n",
      "Prediction: 0.81,    Actual Value: 0.84\n",
      "Prediction: 0.89,    Actual Value: 0.85\n",
      "Prediction: 0.96,    Actual Value: 0.99\n",
      "Prediction: 0.92,    Actual Value: 1.00\n",
      "Prediction: 0.91,    Actual Value: 0.99\n",
      "Prediction: 0.89,    Actual Value: 0.97\n",
      "Prediction: 0.81,    Actual Value: 0.62\n",
      "Prediction: 0.79,    Actual Value: 0.59\n",
      "Prediction: 0.73,    Actual Value: 0.61\n",
      "Prediction: 0.71,    Actual Value: 0.63\n",
      "Prediction: 0.73,    Actual Value: 0.82\n",
      "Prediction: 0.67,    Actual Value: 0.82\n",
      "Prediction: 0.70,    Actual Value: 0.75\n",
      "Prediction: 0.68,    Actual Value: 0.80\n",
      "Prediction: 0.70,    Actual Value: 0.88\n",
      "Prediction: 0.67,    Actual Value: 0.86\n",
      "Prediction: 0.64,    Actual Value: 0.77\n",
      "Prediction: 0.63,    Actual Value: 0.80\n",
      "Prediction: 0.62,    Actual Value: 0.82\n",
      "Prediction: 0.66,    Actual Value: 0.83\n",
      "Prediction: 0.58,    Actual Value: 0.53\n",
      "Prediction: 0.59,    Actual Value: 0.53\n",
      "Prediction: 0.58,    Actual Value: 0.50\n",
      "Prediction: 0.59,    Actual Value: 0.53\n",
      "Prediction: 0.54,    Actual Value: 0.51\n",
      "Prediction: 0.60,    Actual Value: 0.54\n",
      "Prediction: 0.60,    Actual Value: 0.55\n",
      "Prediction: 0.63,    Actual Value: 0.50\n",
      "Prediction: 0.61,    Actual Value: 0.44\n",
      "Prediction: 0.62,    Actual Value: 0.76\n",
      "Prediction: 0.65,    Actual Value: 0.77\n",
      "Prediction: 0.68,    Actual Value: 0.77\n",
      "Prediction: 0.63,    Actual Value: 0.79\n",
      "Prediction: 0.65,    Actual Value: 0.74\n",
      "Prediction: 0.69,    Actual Value: 0.88\n",
      "Prediction: 0.75,    Actual Value: 0.86\n",
      "Prediction: 0.72,    Actual Value: 0.85\n",
      "Prediction: 0.68,    Actual Value: 0.85\n",
      "Prediction: 0.69,    Actual Value: 0.85\n",
      "Prediction: 0.59,    Actual Value: 0.52\n",
      "Prediction: 0.63,    Actual Value: 0.56\n",
      "Prediction: 0.58,    Actual Value: 0.56\n",
      "Prediction: 0.57,    Actual Value: 0.63\n",
      "Prediction: 0.55,    Actual Value: 0.65\n",
      "Prediction: 0.62,    Actual Value: 0.38\n",
      "Prediction: 0.64,    Actual Value: 0.56\n",
      "Prediction: 0.67,    Actual Value: 0.56\n",
      "Prediction: 0.63,    Actual Value: 0.58\n",
      "Prediction: 0.63,    Actual Value: 0.59\n",
      "Prediction: 0.61,    Actual Value: 0.63\n",
      "Prediction: 0.61,    Actual Value: 0.63\n",
      "Prediction: 0.58,    Actual Value: 0.62\n",
      "Prediction: 0.56,    Actual Value: 0.53\n",
      "Prediction: 0.50,    Actual Value: 0.50\n",
      "Prediction: 0.48,    Actual Value: 0.38\n",
      "Prediction: 0.45,    Actual Value: 0.26\n",
      "Prediction: 0.49,    Actual Value: 0.25\n",
      "Prediction: 0.51,    Actual Value: 0.51\n",
      "Prediction: 0.55,    Actual Value: 0.50\n",
      "Prediction: 0.56,    Actual Value: 0.47\n",
      "Prediction: 0.59,    Actual Value: 0.45\n",
      "Prediction: 0.65,    Actual Value: 0.71\n",
      "Prediction: 0.62,    Actual Value: 0.72\n",
      "Prediction: 0.67,    Actual Value: 0.81\n",
      "Prediction: 0.63,    Actual Value: 0.82\n",
      "Prediction: 0.72,    Actual Value: 0.82\n",
      "Prediction: 0.71,    Actual Value: 0.87\n",
      "Prediction: 0.70,    Actual Value: 0.73\n",
      "Prediction: 0.62,    Actual Value: 0.77\n",
      "Prediction: 0.65,    Actual Value: 0.79\n",
      "Prediction: 0.61,    Actual Value: 0.79\n",
      "Prediction: 0.59,    Actual Value: 0.65\n",
      "Prediction: 0.52,    Actual Value: 0.65\n",
      "Prediction: 0.52,    Actual Value: 0.57\n",
      "Prediction: 0.55,    Actual Value: 0.58\n",
      "Prediction: 0.58,    Actual Value: 0.59\n",
      "Prediction: 0.51,    Actual Value: 0.45\n",
      "Prediction: 0.50,    Actual Value: 0.48\n",
      "Prediction: 0.48,    Actual Value: 0.47\n",
      "Prediction: 0.50,    Actual Value: 0.36\n",
      "Prediction: 0.50,    Actual Value: 0.38\n",
      "Prediction: 0.48,    Actual Value: 0.40\n",
      "Prediction: 0.48,    Actual Value: 0.49\n",
      "Prediction: 0.45,    Actual Value: 0.52\n",
      "Prediction: 0.48,    Actual Value: 0.47\n",
      "Prediction: 0.48,    Actual Value: 0.50\n",
      "Prediction: 0.53,    Actual Value: 0.57\n",
      "Prediction: 0.50,    Actual Value: 0.56\n",
      "Prediction: 0.53,    Actual Value: 0.63\n",
      "Prediction: 0.55,    Actual Value: 0.63\n",
      "Prediction: 0.55,    Actual Value: 0.62\n",
      "Prediction: 0.55,    Actual Value: 0.63\n",
      "Prediction: 0.50,    Actual Value: 0.57\n",
      "Prediction: 0.53,    Actual Value: 0.62\n",
      "Prediction: 0.59,    Actual Value: 0.64\n",
      "Prediction: 0.59,    Actual Value: 0.73\n",
      "Prediction: 0.54,    Actual Value: 0.70\n",
      "Prediction: 0.55,    Actual Value: 0.70\n",
      "Prediction: 0.56,    Actual Value: 0.62\n",
      "Prediction: 0.60,    Actual Value: 0.64\n",
      "Prediction: 0.54,    Actual Value: 0.64\n",
      "Prediction: 0.52,    Actual Value: 0.68\n",
      "Prediction: 0.50,    Actual Value: 0.64\n",
      "Prediction: 0.51,    Actual Value: 0.54\n",
      "Prediction: 0.46,    Actual Value: 0.54\n",
      "Prediction: 0.43,    Actual Value: 0.38\n",
      "Prediction: 0.43,    Actual Value: 0.43\n",
      "Prediction: 0.47,    Actual Value: 0.45\n",
      "Prediction: 0.45,    Actual Value: 0.45\n",
      "Prediction: 0.45,    Actual Value: 0.43\n",
      "Prediction: 0.42,    Actual Value: 0.49\n",
      "Prediction: 0.41,    Actual Value: 0.44\n",
      "Prediction: 0.42,    Actual Value: 0.46\n",
      "Prediction: 0.40,    Actual Value: 0.46\n",
      "Prediction: 0.38,    Actual Value: 0.46\n",
      "Prediction: 0.41,    Actual Value: 0.41\n",
      "Prediction: 0.38,    Actual Value: 0.37\n",
      "Prediction: 0.40,    Actual Value: 0.37\n",
      "Prediction: 0.38,    Actual Value: 0.38\n",
      "Prediction: 0.40,    Actual Value: 0.37\n",
      "Prediction: 0.40,    Actual Value: 0.32\n",
      "Prediction: 0.35,    Actual Value: 0.30\n",
      "Prediction: 0.36,    Actual Value: 0.30\n",
      "Prediction: 0.34,    Actual Value: 0.29\n",
      "Prediction: 0.37,    Actual Value: 0.32\n",
      "Prediction: 0.32,    Actual Value: 0.31\n",
      "Prediction: 0.34,    Actual Value: 0.32\n",
      "Prediction: 0.28,    Actual Value: 0.32\n",
      "Prediction: 0.36,    Actual Value: 0.40\n",
      "Prediction: 0.32,    Actual Value: 0.40\n",
      "Prediction: 0.31,    Actual Value: 0.35\n",
      "Prediction: 0.30,    Actual Value: 0.37\n",
      "Prediction: 0.32,    Actual Value: 0.36\n",
      "Prediction: 0.34,    Actual Value: 0.37\n",
      "Prediction: 0.34,    Actual Value: 0.35\n",
      "Prediction: 0.29,    Actual Value: 0.39\n",
      "Prediction: 0.28,    Actual Value: 0.39\n",
      "Prediction: 0.35,    Actual Value: 0.44\n",
      "Prediction: 0.35,    Actual Value: 0.32\n",
      "Prediction: 0.33,    Actual Value: 0.33\n",
      "Prediction: 0.33,    Actual Value: 0.45\n",
      "Prediction: 0.38,    Actual Value: 0.45\n",
      "Prediction: 0.40,    Actual Value: 0.46\n",
      "Prediction: 0.48,    Actual Value: 0.46\n",
      "Prediction: 0.42,    Actual Value: 0.48\n",
      "Prediction: 0.47,    Actual Value: 0.47\n",
      "Prediction: 0.43,    Actual Value: 0.53\n",
      "Prediction: 0.50,    Actual Value: 0.52\n",
      "Prediction: 0.52,    Actual Value: 0.57\n",
      "Prediction: 0.54,    Actual Value: 0.57\n",
      "Prediction: 0.45,    Actual Value: 0.48\n",
      "Prediction: 0.47,    Actual Value: 0.57\n",
      "Prediction: 0.43,    Actual Value: 0.55\n",
      "Prediction: 0.49,    Actual Value: 0.55\n",
      "Prediction: 0.49,    Actual Value: 0.53\n",
      "Prediction: 0.42,    Actual Value: 0.55\n",
      "Prediction: 0.42,    Actual Value: 0.46\n",
      "Prediction: 0.36,    Actual Value: 0.42\n",
      "Prediction: 0.34,    Actual Value: 0.38\n",
      "Prediction: 0.33,    Actual Value: 0.39\n",
      "Prediction: 0.34,    Actual Value: 0.39\n",
      "Prediction: 0.36,    Actual Value: 0.26\n",
      "Prediction: 0.39,    Actual Value: 0.26\n",
      "Prediction: 0.40,    Actual Value: 0.24\n",
      "Prediction: 0.41,    Actual Value: 0.41\n",
      "Prediction: 0.40,    Actual Value: 0.41\n",
      "Prediction: 0.41,    Actual Value: 0.43\n",
      "Prediction: 0.36,    Actual Value: 0.47\n",
      "Prediction: 0.41,    Actual Value: 0.47\n",
      "Prediction: 0.37,    Actual Value: 0.45\n",
      "Prediction: 0.40,    Actual Value: 0.49\n",
      "Prediction: 0.38,    Actual Value: 0.49\n",
      "Prediction: 0.41,    Actual Value: 0.49\n",
      "Prediction: 0.45,    Actual Value: 0.49\n",
      "Prediction: 0.47,    Actual Value: 0.38\n",
      "Prediction: 0.38,    Actual Value: 0.33\n",
      "Prediction: 0.43,    Actual Value: 0.34\n",
      "Prediction: 0.39,    Actual Value: 0.30\n",
      "Prediction: 0.41,    Actual Value: 0.49\n",
      "Prediction: 0.48,    Actual Value: 0.61\n",
      "Prediction: 0.51,    Actual Value: 0.63\n",
      "Prediction: 0.55,    Actual Value: 0.62\n",
      "Prediction: 0.50,    Actual Value: 0.62\n",
      "Prediction: 0.55,    Actual Value: 0.67\n",
      "Prediction: 0.51,    Actual Value: 0.63\n",
      "Prediction: 0.56,    Actual Value: 0.67\n",
      "Prediction: 0.58,    Actual Value: 0.67\n",
      "Prediction: 0.66,    Actual Value: 0.71\n",
      "Prediction: 0.51,    Actual Value: 0.61\n",
      "Prediction: 0.50,    Actual Value: 0.44\n",
      "Prediction: 0.47,    Actual Value: 0.44\n",
      "Prediction: 0.48,    Actual Value: 0.51\n",
      "Prediction: 0.48,    Actual Value: 0.53\n",
      "Prediction: 0.47,    Actual Value: 0.48\n",
      "Prediction: 0.45,    Actual Value: 0.47\n",
      "Prediction: 0.46,    Actual Value: 0.43\n",
      "Prediction: 0.51,    Actual Value: 0.42\n",
      "Prediction: 0.52,    Actual Value: 0.35\n",
      "Prediction: 0.45,    Actual Value: 0.35\n",
      "Prediction: 0.39,    Actual Value: 0.36\n",
      "Prediction: 0.40,    Actual Value: 0.36\n",
      "Prediction: 0.27,    Actual Value: 0.21\n",
      "Prediction: 0.27,    Actual Value: 0.21\n",
      "Prediction: 0.28,    Actual Value: 0.26\n",
      "Prediction: 0.38,    Actual Value: 0.26\n",
      "Prediction: 0.28,    Actual Value: 0.26\n",
      "Prediction: 0.30,    Actual Value: 0.34\n",
      "Prediction: 0.28,    Actual Value: 0.35\n",
      "Prediction: 0.30,    Actual Value: 0.35\n",
      "Prediction: 0.34,    Actual Value: 0.39\n",
      "Prediction: 0.29,    Actual Value: 0.38\n",
      "Prediction: 0.36,    Actual Value: 0.38\n",
      "Prediction: 0.34,    Actual Value: 0.41\n",
      "Prediction: 0.33,    Actual Value: 0.37\n",
      "Prediction: 0.31,    Actual Value: 0.41\n",
      "Prediction: 0.34,    Actual Value: 0.41\n",
      "Prediction: 0.35,    Actual Value: 0.34\n",
      "Prediction: 0.30,    Actual Value: 0.36\n",
      "Prediction: 0.26,    Actual Value: 0.40\n",
      "Prediction: 0.23,    Actual Value: 0.35\n",
      "Prediction: 0.28,    Actual Value: 0.36\n",
      "Prediction: 0.33,    Actual Value: 0.36\n",
      "Prediction: 0.30,    Actual Value: 0.34\n",
      "Prediction: 0.32,    Actual Value: 0.39\n",
      "Prediction: 0.40,    Actual Value: 0.49\n",
      "Prediction: 0.36,    Actual Value: 0.50\n",
      "Prediction: 0.37,    Actual Value: 0.50\n",
      "Prediction: 0.42,    Actual Value: 0.49\n",
      "Prediction: 0.50,    Actual Value: 0.46\n",
      "Prediction: 0.43,    Actual Value: 0.48\n",
      "Prediction: 0.45,    Actual Value: 0.48\n",
      "Prediction: 0.41,    Actual Value: 0.48\n",
      "Prediction: 0.36,    Actual Value: 0.47\n",
      "Prediction: 0.39,    Actual Value: 0.45\n",
      "Prediction: 0.39,    Actual Value: 0.32\n",
      "Prediction: 0.43,    Actual Value: 0.32\n",
      "Prediction: 0.43,    Actual Value: 0.36\n",
      "Prediction: 0.47,    Actual Value: 0.43\n",
      "Prediction: 0.45,    Actual Value: 0.41\n",
      "Prediction: 0.49,    Actual Value: 0.41\n",
      "Prediction: 0.47,    Actual Value: 0.45\n",
      "Prediction: 0.52,    Actual Value: 0.46\n",
      "Prediction: 0.54,    Actual Value: 0.48\n",
      "Prediction: 0.58,    Actual Value: 0.91\n",
      "Prediction: 0.61,    Actual Value: 0.90\n",
      "Prediction: 0.63,    Actual Value: 0.90\n",
      "Prediction: 0.66,    Actual Value: 0.90\n",
      "Prediction: 0.67,    Actual Value: 0.88\n",
      "Prediction: 0.66,    Actual Value: 0.87\n",
      "Prediction: 0.71,    Actual Value: 0.83\n",
      "Prediction: 0.71,    Actual Value: 0.86\n",
      "Prediction: 0.66,    Actual Value: 0.83\n",
      "Prediction: 0.60,    Actual Value: 0.84\n",
      "Prediction: 0.54,    Actual Value: 0.46\n",
      "Prediction: 0.58,    Actual Value: 0.51\n",
      "Prediction: 0.63,    Actual Value: 0.51\n",
      "Prediction: 0.58,    Actual Value: 0.65\n",
      "Prediction: 0.55,    Actual Value: 0.63\n",
      "Prediction: 0.61,    Actual Value: 0.63\n",
      "Prediction: 0.58,    Actual Value: 0.65\n",
      "Prediction: 0.62,    Actual Value: 0.62\n",
      "Prediction: 0.61,    Actual Value: 0.62\n",
      "Prediction: 0.61,    Actual Value: 0.63\n",
      "Prediction: 0.63,    Actual Value: 0.63\n",
      "Prediction: 0.60,    Actual Value: 0.57\n",
      "Prediction: 0.58,    Actual Value: 0.58\n",
      "Prediction: 0.55,    Actual Value: 0.43\n",
      "Prediction: 0.55,    Actual Value: 0.44\n",
      "Prediction: 0.55,    Actual Value: 0.44\n",
      "Prediction: 0.46,    Actual Value: 0.39\n",
      "Prediction: 0.47,    Actual Value: 0.42\n",
      "Prediction: 0.45,    Actual Value: 0.39\n",
      "Prediction: 0.54,    Actual Value: 0.38\n",
      "Prediction: 0.45,    Actual Value: 0.42\n",
      "Prediction: 0.51,    Actual Value: 0.41\n",
      "Prediction: 0.46,    Actual Value: 0.40\n",
      "Prediction: 0.49,    Actual Value: 0.45\n",
      "Prediction: 0.51,    Actual Value: 0.47\n",
      "Prediction: 0.56,    Actual Value: 0.46\n",
      "Prediction: 0.60,    Actual Value: 0.66\n",
      "Prediction: 0.72,    Actual Value: 0.69\n",
      "Prediction: 0.77,    Actual Value: 0.66\n",
      "Prediction: 0.82,    Actual Value: 0.69\n",
      "Prediction: 0.95,    Actual Value: 1.13\n",
      "Prediction: 1.01,    Actual Value: 1.13\n",
      "Prediction: 1.02,    Actual Value: 1.17\n",
      "Prediction: 1.05,    Actual Value: 1.17\n",
      "Prediction: 1.06,    Actual Value: 1.20\n",
      "Prediction: 1.06,    Actual Value: 1.20\n",
      "Prediction: 1.00,    Actual Value: 1.10\n",
      "Prediction: 0.94,    Actual Value: 0.98\n",
      "Prediction: 0.87,    Actual Value: 0.98\n",
      "Prediction: 0.80,    Actual Value: 0.88\n",
      "Prediction: 0.78,    Actual Value: 0.51\n",
      "Prediction: 0.78,    Actual Value: 0.51\n",
      "Prediction: 0.80,    Actual Value: 0.70\n",
      "Prediction: 0.78,    Actual Value: 0.65\n",
      "Prediction: 0.76,    Actual Value: 0.72\n",
      "Prediction: 0.78,    Actual Value: 0.70\n",
      "Prediction: 0.76,    Actual Value: 0.73\n",
      "Prediction: 0.76,    Actual Value: 0.73\n",
      "Prediction: 0.76,    Actual Value: 0.74\n",
      "Prediction: 0.71,    Actual Value: 0.66\n",
      "Prediction: 0.69,    Actual Value: 0.68\n",
      "Prediction: 0.64,    Actual Value: 0.67\n",
      "Prediction: 0.64,    Actual Value: 0.56\n",
      "Prediction: 0.67,    Actual Value: 0.68\n",
      "Prediction: 0.74,    Actual Value: 0.62\n",
      "Prediction: 0.75,    Actual Value: 0.68\n",
      "Prediction: 0.74,    Actual Value: 0.70\n",
      "Prediction: 0.71,    Actual Value: 0.69\n",
      "Prediction: 0.69,    Actual Value: 0.66\n",
      "Prediction: 0.73,    Actual Value: 0.77\n",
      "Prediction: 0.77,    Actual Value: 0.83\n",
      "Prediction: 0.71,    Actual Value: 0.85\n",
      "Prediction: 0.68,    Actual Value: 0.81\n",
      "Prediction: 0.61,    Actual Value: 0.68\n",
      "Prediction: 0.63,    Actual Value: 0.65\n",
      "Prediction: 0.58,    Actual Value: 0.49\n",
      "Prediction: 0.56,    Actual Value: 0.49\n",
      "Prediction: 0.47,    Actual Value: 0.49\n",
      "Prediction: 0.48,    Actual Value: 0.51\n",
      "Prediction: 0.49,    Actual Value: 0.42\n",
      "Prediction: 0.45,    Actual Value: 0.32\n",
      "Prediction: 0.45,    Actual Value: 0.31\n",
      "Prediction: 0.41,    Actual Value: 0.30\n",
      "Prediction: 0.40,    Actual Value: 0.26\n",
      "Prediction: 0.39,    Actual Value: 0.27\n",
      "Prediction: 0.37,    Actual Value: 0.26\n",
      "Prediction: 0.34,    Actual Value: 0.26\n",
      "Prediction: 0.41,    Actual Value: 0.26\n",
      "Prediction: 0.41,    Actual Value: 0.32\n",
      "Prediction: 0.39,    Actual Value: 0.35\n",
      "Prediction: 0.40,    Actual Value: 0.36\n",
      "Prediction: 0.44,    Actual Value: 0.41\n",
      "Prediction: 0.41,    Actual Value: 0.45\n",
      "Prediction: 0.49,    Actual Value: 0.52\n",
      "Prediction: 0.49,    Actual Value: 0.55\n",
      "Prediction: 0.55,    Actual Value: 0.58\n",
      "Prediction: 0.57,    Actual Value: 0.70\n",
      "Prediction: 0.65,    Actual Value: 0.82\n",
      "Prediction: 0.70,    Actual Value: 0.74\n",
      "Prediction: 0.75,    Actual Value: 0.77\n",
      "Prediction: 0.77,    Actual Value: 0.76\n",
      "Prediction: 0.68,    Actual Value: 0.79\n",
      "Prediction: 0.68,    Actual Value: 0.78\n",
      "Prediction: 0.65,    Actual Value: 0.74\n",
      "Prediction: 0.66,    Actual Value: 0.75\n",
      "Prediction: 0.59,    Actual Value: 0.72\n",
      "Prediction: 0.55,    Actual Value: 0.50\n",
      "Prediction: 0.51,    Actual Value: 0.39\n",
      "Prediction: 0.50,    Actual Value: 0.38\n",
      "Prediction: 0.49,    Actual Value: 0.49\n",
      "Prediction: 0.44,    Actual Value: 0.38\n",
      "Prediction: 0.46,    Actual Value: 0.37\n",
      "Prediction: 0.47,    Actual Value: 0.40\n",
      "Prediction: 0.45,    Actual Value: 0.45\n",
      "Prediction: 0.44,    Actual Value: 0.43\n",
      "Prediction: 0.43,    Actual Value: 0.43\n",
      "Prediction: 0.39,    Actual Value: 0.40\n",
      "Prediction: 0.38,    Actual Value: 0.41\n",
      "Prediction: 0.40,    Actual Value: 0.42\n",
      "Prediction: 0.37,    Actual Value: 0.29\n",
      "Prediction: 0.38,    Actual Value: 0.40\n",
      "Prediction: 0.37,    Actual Value: 0.43\n",
      "Prediction: 0.37,    Actual Value: 0.41\n",
      "Prediction: 0.39,    Actual Value: 0.34\n",
      "Prediction: 0.35,    Actual Value: 0.44\n",
      "Prediction: 0.36,    Actual Value: 0.46\n",
      "Prediction: 0.39,    Actual Value: 0.46\n",
      "Prediction: 0.39,    Actual Value: 0.45\n",
      "Prediction: 0.41,    Actual Value: 0.45\n",
      "Prediction: 0.43,    Actual Value: 0.47\n",
      "Prediction: 0.37,    Actual Value: 0.43\n",
      "Prediction: 0.44,    Actual Value: 0.40\n",
      "Prediction: 0.45,    Actual Value: 0.47\n",
      "Prediction: 0.49,    Actual Value: 0.47\n",
      "Prediction: 0.41,    Actual Value: 0.43\n",
      "Prediction: 0.43,    Actual Value: 0.41\n",
      "Prediction: 0.38,    Actual Value: 0.43\n",
      "Prediction: 0.45,    Actual Value: 0.45\n",
      "Prediction: 0.45,    Actual Value: 0.51\n",
      "Prediction: 0.46,    Actual Value: 0.50\n",
      "Prediction: 0.46,    Actual Value: 0.58\n",
      "Prediction: 0.46,    Actual Value: 0.56\n",
      "Prediction: 0.45,    Actual Value: 0.53\n",
      "Prediction: 0.48,    Actual Value: 0.54\n",
      "Prediction: 0.50,    Actual Value: 0.55\n",
      "Prediction: 0.48,    Actual Value: 0.59\n",
      "Prediction: 0.47,    Actual Value: 0.57\n",
      "Prediction: 0.46,    Actual Value: 0.57\n",
      "Prediction: 0.48,    Actual Value: 0.51\n",
      "Prediction: 0.48,    Actual Value: 0.62\n",
      "Prediction: 0.49,    Actual Value: 0.46\n",
      "Prediction: 0.50,    Actual Value: 0.47\n",
      "Prediction: 0.55,    Actual Value: 0.47\n",
      "Prediction: 0.50,    Actual Value: 0.65\n",
      "Prediction: 0.49,    Actual Value: 0.68\n",
      "Prediction: 0.52,    Actual Value: 0.66\n",
      "Prediction: 0.63,    Actual Value: 0.83\n",
      "Prediction: 0.65,    Actual Value: 0.83\n",
      "Prediction: 0.64,    Actual Value: 0.80\n",
      "Prediction: 0.62,    Actual Value: 0.75\n",
      "Prediction: 0.61,    Actual Value: 0.76\n",
      "Prediction: 0.64,    Actual Value: 0.76\n",
      "Prediction: 0.72,    Actual Value: 0.78\n",
      "Prediction: 0.71,    Actual Value: 0.63\n",
      "Prediction: 0.61,    Actual Value: 0.61\n",
      "Prediction: 0.60,    Actual Value: 0.61\n",
      "Prediction: 0.56,    Actual Value: 0.60\n",
      "Prediction: 0.62,    Actual Value: 0.59\n",
      "Prediction: 0.66,    Actual Value: 0.59\n",
      "Prediction: 0.68,    Actual Value: 0.64\n",
      "Prediction: 0.69,    Actual Value: 0.70\n",
      "Prediction: 0.74,    Actual Value: 0.85\n",
      "Prediction: 0.74,    Actual Value: 0.83\n",
      "Prediction: 0.79,    Actual Value: 0.81\n",
      "Prediction: 0.76,    Actual Value: 0.81\n",
      "Prediction: 0.76,    Actual Value: 0.81\n",
      "Prediction: 0.67,    Actual Value: 0.57\n",
      "Prediction: 0.66,    Actual Value: 0.55\n",
      "Prediction: 0.62,    Actual Value: 0.62\n",
      "Prediction: 0.61,    Actual Value: 0.65\n",
      "Prediction: 0.64,    Actual Value: 0.69\n",
      "Prediction: 0.64,    Actual Value: 0.49\n",
      "Prediction: 0.66,    Actual Value: 0.50\n",
      "Prediction: 0.63,    Actual Value: 0.53\n",
      "Prediction: 0.75,    Actual Value: 0.76\n",
      "Prediction: 0.69,    Actual Value: 0.77\n",
      "Prediction: 0.67,    Actual Value: 0.82\n",
      "Prediction: 0.72,    Actual Value: 0.80\n",
      "Prediction: 0.68,    Actual Value: 0.77\n",
      "Prediction: 0.73,    Actual Value: 0.86\n",
      "Prediction: 0.68,    Actual Value: 0.81\n",
      "Prediction: 0.71,    Actual Value: 0.83\n",
      "Prediction: 0.66,    Actual Value: 0.83\n",
      "Prediction: 0.67,    Actual Value: 0.80\n",
      "Prediction: 0.60,    Actual Value: 0.53\n",
      "Prediction: 0.66,    Actual Value: 0.62\n",
      "Prediction: 0.61,    Actual Value: 0.57\n",
      "Prediction: 0.51,    Actual Value: 0.56\n",
      "Prediction: 0.55,    Actual Value: 0.56\n",
      "Prediction: 0.41,    Actual Value: 0.43\n",
      "Prediction: 0.48,    Actual Value: 0.47\n",
      "Prediction: 0.51,    Actual Value: 0.66\n",
      "Prediction: 0.54,    Actual Value: 0.66\n",
      "Prediction: 0.57,    Actual Value: 0.72\n",
      "Prediction: 0.66,    Actual Value: 0.74\n",
      "Prediction: 0.70,    Actual Value: 0.68\n",
      "Prediction: 0.65,    Actual Value: 0.68\n",
      "Prediction: 0.74,    Actual Value: 0.71\n",
      "Prediction: 0.73,    Actual Value: 0.82\n",
      "Prediction: 0.80,    Actual Value: 0.82\n",
      "Prediction: 0.72,    Actual Value: 0.80\n",
      "Prediction: 0.68,    Actual Value: 0.56\n",
      "Prediction: 0.64,    Actual Value: 0.70\n",
      "Prediction: 0.78,    Actual Value: 0.68\n",
      "Prediction: 0.70,    Actual Value: 0.70\n",
      "Prediction: 0.68,    Actual Value: 0.69\n",
      "Prediction: 0.67,    Actual Value: 0.78\n",
      "Prediction: 0.63,    Actual Value: 0.68\n",
      "Prediction: 0.73,    Actual Value: 0.65\n",
      "Prediction: 0.60,    Actual Value: 0.66\n",
      "Prediction: 0.62,    Actual Value: 0.67\n",
      "Prediction: 0.60,    Actual Value: 0.65\n",
      "Prediction: 0.53,    Actual Value: 0.41\n",
      "Prediction: 0.44,    Actual Value: 0.42\n",
      "Prediction: 0.47,    Actual Value: 0.42\n",
      "Prediction: 0.45,    Actual Value: 0.47\n",
      "Prediction: 0.42,    Actual Value: 0.37\n",
      "Prediction: 0.38,    Actual Value: 0.41\n",
      "Prediction: 0.32,    Actual Value: 0.33\n",
      "Prediction: 0.37,    Actual Value: 0.34\n",
      "Prediction: 0.34,    Actual Value: 0.33\n",
      "Prediction: 0.26,    Actual Value: 0.29\n",
      "Prediction: 0.27,    Actual Value: 0.29\n",
      "Prediction: 0.34,    Actual Value: 0.29\n",
      "Prediction: 0.34,    Actual Value: 0.23\n",
      "Prediction: 0.33,    Actual Value: 0.19\n",
      "Prediction: 0.35,    Actual Value: 0.35\n",
      "Prediction: 0.37,    Actual Value: 0.35\n",
      "Prediction: 0.39,    Actual Value: 0.34\n",
      "Prediction: 0.40,    Actual Value: 0.35\n",
      "Prediction: 0.38,    Actual Value: 0.42\n",
      "Prediction: 0.40,    Actual Value: 0.47\n",
      "Prediction: 0.43,    Actual Value: 0.47\n",
      "Prediction: 0.41,    Actual Value: 0.47\n",
      "Prediction: 0.41,    Actual Value: 0.47\n",
      "Prediction: 0.43,    Actual Value: 0.47\n",
      "Prediction: 0.40,    Actual Value: 0.40\n",
      "Prediction: 0.44,    Actual Value: 0.36\n",
      "Prediction: 0.44,    Actual Value: 0.38\n",
      "Prediction: 0.47,    Actual Value: 0.38\n",
      "Prediction: 0.48,    Actual Value: 0.34\n",
      "Prediction: 0.44,    Actual Value: 0.43\n",
      "Prediction: 0.43,    Actual Value: 0.43\n",
      "Prediction: 0.42,    Actual Value: 0.44\n",
      "Prediction: 0.45,    Actual Value: 0.44\n",
      "Prediction: 0.48,    Actual Value: 0.47\n",
      "Prediction: 0.46,    Actual Value: 0.47\n",
      "Prediction: 0.40,    Actual Value: 0.42\n",
      "Prediction: 0.38,    Actual Value: 0.42\n",
      "Prediction: 0.39,    Actual Value: 0.41\n",
      "Prediction: 0.36,    Actual Value: 0.40\n",
      "Prediction: 0.31,    Actual Value: 0.34\n",
      "Prediction: 0.32,    Actual Value: 0.30\n",
      "Prediction: 0.39,    Actual Value: 0.33\n",
      "Prediction: 0.39,    Actual Value: 0.34\n",
      "Prediction: 0.36,    Actual Value: 0.32\n",
      "Prediction: 0.39,    Actual Value: 0.46\n",
      "Prediction: 0.44,    Actual Value: 0.51\n",
      "Prediction: 0.44,    Actual Value: 0.53\n",
      "Prediction: 0.43,    Actual Value: 0.52\n",
      "Prediction: 0.45,    Actual Value: 0.51\n",
      "Prediction: 0.44,    Actual Value: 0.46\n",
      "Prediction: 0.48,    Actual Value: 0.52\n",
      "Prediction: 0.49,    Actual Value: 0.50\n",
      "Prediction: 0.47,    Actual Value: 0.48\n",
      "Prediction: 0.50,    Actual Value: 0.65\n",
      "Prediction: 0.51,    Actual Value: 0.56\n",
      "Prediction: 0.51,    Actual Value: 0.54\n",
      "Prediction: 0.53,    Actual Value: 0.53\n",
      "Prediction: 0.54,    Actual Value: 0.55\n",
      "Prediction: 0.54,    Actual Value: 0.56\n",
      "Prediction: 0.50,    Actual Value: 0.55\n",
      "Prediction: 0.46,    Actual Value: 0.48\n",
      "Prediction: 0.45,    Actual Value: 0.48\n",
      "Prediction: 0.46,    Actual Value: 0.48\n",
      "Prediction: 0.45,    Actual Value: 0.40\n",
      "Prediction: 0.42,    Actual Value: 0.41\n",
      "Prediction: 0.38,    Actual Value: 0.39\n",
      "Prediction: 0.43,    Actual Value: 0.39\n",
      "Prediction: 0.43,    Actual Value: 0.36\n",
      "Prediction: 0.41,    Actual Value: 0.39\n",
      "Prediction: 0.43,    Actual Value: 0.39\n",
      "Prediction: 0.39,    Actual Value: 0.41\n",
      "Prediction: 0.40,    Actual Value: 0.41\n",
      "Prediction: 0.38,    Actual Value: 0.41\n",
      "Prediction: 0.36,    Actual Value: 0.29\n",
      "Prediction: 0.38,    Actual Value: 0.27\n",
      "Prediction: 0.38,    Actual Value: 0.29\n",
      "Prediction: 0.35,    Actual Value: 0.30\n",
      "Prediction: 0.30,    Actual Value: 0.30\n",
      "Prediction: 0.29,    Actual Value: 0.36\n",
      "Prediction: 0.30,    Actual Value: 0.36\n",
      "Prediction: 0.36,    Actual Value: 0.30\n",
      "Prediction: 0.41,    Actual Value: 0.30\n",
      "Prediction: 0.42,    Actual Value: 0.36\n",
      "Prediction: 0.41,    Actual Value: 0.44\n",
      "Prediction: 0.43,    Actual Value: 0.45\n",
      "Prediction: 0.44,    Actual Value: 0.43\n",
      "Prediction: 0.45,    Actual Value: 0.42\n",
      "Prediction: 0.45,    Actual Value: 0.42\n",
      "Prediction: 0.41,    Actual Value: 0.34\n",
      "Prediction: 0.43,    Actual Value: 0.32\n",
      "Prediction: 0.44,    Actual Value: 0.41\n",
      "Prediction: 0.46,    Actual Value: 0.42\n",
      "Prediction: 0.38,    Actual Value: 0.42\n",
      "Prediction: 0.43,    Actual Value: 0.40\n",
      "Prediction: 0.43,    Actual Value: 0.39\n",
      "Prediction: 0.48,    Actual Value: 0.39\n",
      "Prediction: 0.45,    Actual Value: 0.39\n",
      "Prediction: 0.47,    Actual Value: 0.39\n",
      "Prediction: 0.45,    Actual Value: 0.38\n",
      "Prediction: 0.49,    Actual Value: 0.38\n",
      "Prediction: 0.49,    Actual Value: 0.28\n",
      "Prediction: 0.46,    Actual Value: 0.57\n",
      "Prediction: 0.54,    Actual Value: 0.57\n",
      "Prediction: 0.57,    Actual Value: 0.59\n",
      "Prediction: 0.59,    Actual Value: 0.66\n",
      "Prediction: 0.58,    Actual Value: 0.66\n",
      "Prediction: 0.60,    Actual Value: 0.66\n",
      "Prediction: 0.61,    Actual Value: 0.73\n",
      "Prediction: 0.68,    Actual Value: 0.71\n",
      "Prediction: 0.71,    Actual Value: 0.94\n",
      "Prediction: 0.76,    Actual Value: 0.93\n",
      "Prediction: 0.71,    Actual Value: 0.77\n",
      "Prediction: 0.74,    Actual Value: 0.77\n",
      "Prediction: 0.78,    Actual Value: 0.75\n",
      "Prediction: 0.79,    Actual Value: 0.73\n",
      "Prediction: 0.65,    Actual Value: 0.74\n",
      "Prediction: 0.60,    Actual Value: 0.79\n",
      "Prediction: 0.57,    Actual Value: 0.73\n",
      "Prediction: 0.61,    Actual Value: 0.78\n",
      "Prediction: 0.59,    Actual Value: 0.53\n",
      "Prediction: 0.60,    Actual Value: 0.53\n",
      "Prediction: 0.57,    Actual Value: 0.54\n",
      "Prediction: 0.49,    Actual Value: 0.59\n",
      "Prediction: 0.48,    Actual Value: 0.59\n",
      "Prediction: 0.56,    Actual Value: 0.56\n",
      "Prediction: 0.67,    Actual Value: 0.64\n",
      "Prediction: 0.55,    Actual Value: 0.54\n",
      "Prediction: 0.59,    Actual Value: 0.70\n",
      "Prediction: 0.55,    Actual Value: 0.67\n",
      "Prediction: 0.59,    Actual Value: 0.68\n",
      "Prediction: 0.57,    Actual Value: 0.68\n",
      "Prediction: 0.55,    Actual Value: 0.67\n",
      "Prediction: 0.55,    Actual Value: 0.61\n",
      "Prediction: 0.67,    Actual Value: 0.61\n",
      "Prediction: 0.72,    Actual Value: 0.70\n",
      "Prediction: 0.70,    Actual Value: 0.60\n",
      "Prediction: 0.68,    Actual Value: 0.58\n",
      "Prediction: 0.65,    Actual Value: 0.47\n",
      "Prediction: 0.63,    Actual Value: 0.51\n",
      "Prediction: 0.62,    Actual Value: 0.52\n",
      "Prediction: 0.49,    Actual Value: 0.53\n",
      "Prediction: 0.56,    Actual Value: 0.53\n",
      "Prediction: 0.66,    Actual Value: 0.63\n",
      "Prediction: 0.70,    Actual Value: 0.80\n",
      "Prediction: 0.70,    Actual Value: 0.71\n",
      "Prediction: 0.71,    Actual Value: 0.69\n",
      "Prediction: 0.69,    Actual Value: 0.71\n",
      "Prediction: 0.71,    Actual Value: 0.74\n",
      "Prediction: 0.69,    Actual Value: 0.65\n",
      "Prediction: 0.70,    Actual Value: 0.68\n",
      "Prediction: 0.65,    Actual Value: 0.70\n",
      "Prediction: 0.60,    Actual Value: 0.68\n",
      "Prediction: 0.56,    Actual Value: 0.65\n",
      "Prediction: 0.54,    Actual Value: 0.44\n",
      "Prediction: 0.53,    Actual Value: 0.43\n",
      "Prediction: 0.47,    Actual Value: 0.43\n",
      "Prediction: 0.47,    Actual Value: 0.40\n",
      "Prediction: 0.42,    Actual Value: 0.31\n",
      "Prediction: 0.39,    Actual Value: 0.31\n",
      "Prediction: 0.39,    Actual Value: 0.22\n",
      "Prediction: 0.38,    Actual Value: 0.22\n",
      "Prediction: 0.39,    Actual Value: 0.38\n",
      "Prediction: 0.38,    Actual Value: 0.36\n",
      "Prediction: 0.38,    Actual Value: 0.36\n",
      "Prediction: 0.33,    Actual Value: 0.37\n",
      "Prediction: 0.35,    Actual Value: 0.37\n",
      "Prediction: 0.35,    Actual Value: 0.37\n",
      "Prediction: 0.38,    Actual Value: 0.38\n",
      "Prediction: 0.33,    Actual Value: 0.40\n",
      "Prediction: 0.32,    Actual Value: 0.38\n",
      "Prediction: 0.31,    Actual Value: 0.37\n",
      "Prediction: 0.34,    Actual Value: 0.27\n",
      "Prediction: 0.35,    Actual Value: 0.27\n",
      "Prediction: 0.35,    Actual Value: 0.26\n",
      "Prediction: 0.31,    Actual Value: 0.26\n",
      "Prediction: 0.30,    Actual Value: 0.28\n",
      "Prediction: 0.32,    Actual Value: 0.37\n",
      "Prediction: 0.34,    Actual Value: 0.37\n",
      "Prediction: 0.33,    Actual Value: 0.31\n",
      "Prediction: 0.27,    Actual Value: 0.30\n",
      "Prediction: 0.29,    Actual Value: 0.30\n",
      "Prediction: 0.27,    Actual Value: 0.26\n",
      "Prediction: 0.34,    Actual Value: 0.26\n",
      "Prediction: 0.32,    Actual Value: 0.29\n",
      "Prediction: 0.32,    Actual Value: 0.32\n",
      "Prediction: 0.31,    Actual Value: 0.35\n",
      "Prediction: 0.32,    Actual Value: 0.28\n",
      "Prediction: 0.35,    Actual Value: 0.28\n",
      "Prediction: 0.34,    Actual Value: 0.40\n",
      "Prediction: 0.34,    Actual Value: 0.42\n",
      "Prediction: 0.30,    Actual Value: 0.42\n",
      "Prediction: 0.36,    Actual Value: 0.41\n",
      "Prediction: 0.41,    Actual Value: 0.40\n",
      "Prediction: 0.39,    Actual Value: 0.40\n",
      "Prediction: 0.33,    Actual Value: 0.41\n",
      "Prediction: 0.39,    Actual Value: 0.43\n",
      "Prediction: 0.39,    Actual Value: 0.43\n",
      "Prediction: 0.42,    Actual Value: 0.39\n",
      "Prediction: 0.40,    Actual Value: 0.35\n",
      "Prediction: 0.35,    Actual Value: 0.34\n",
      "Prediction: 0.39,    Actual Value: 0.41\n",
      "Prediction: 0.38,    Actual Value: 0.41\n",
      "Prediction: 0.40,    Actual Value: 0.43\n",
      "Prediction: 0.41,    Actual Value: 0.39\n",
      "Prediction: 0.41,    Actual Value: 0.39\n",
      "Prediction: 0.40,    Actual Value: 0.37\n",
      "Prediction: 0.42,    Actual Value: 0.42\n",
      "Prediction: 0.46,    Actual Value: 0.41\n",
      "Prediction: 0.44,    Actual Value: 0.43\n",
      "Prediction: 0.45,    Actual Value: 0.48\n",
      "Prediction: 0.38,    Actual Value: 0.43\n",
      "Prediction: 0.43,    Actual Value: 0.43\n",
      "Prediction: 0.49,    Actual Value: 0.50\n",
      "Prediction: 0.51,    Actual Value: 0.55\n",
      "Prediction: 0.50,    Actual Value: 0.57\n",
      "Prediction: 0.54,    Actual Value: 0.61\n",
      "Prediction: 0.55,    Actual Value: 0.57\n",
      "Prediction: 0.57,    Actual Value: 0.60\n",
      "Prediction: 0.59,    Actual Value: 0.65\n",
      "Prediction: 0.63,    Actual Value: 0.63\n",
      "Prediction: 0.61,    Actual Value: 0.62\n",
      "Prediction: 0.55,    Actual Value: 0.63\n",
      "Prediction: 0.51,    Actual Value: 0.64\n",
      "Prediction: 0.47,    Actual Value: 0.50\n",
      "Prediction: 0.41,    Actual Value: 0.50\n",
      "Prediction: 0.48,    Actual Value: 0.49\n",
      "Prediction: 0.49,    Actual Value: 0.51\n",
      "Prediction: 0.44,    Actual Value: 0.55\n",
      "Prediction: 0.45,    Actual Value: 0.50\n",
      "Prediction: 0.36,    Actual Value: 0.51\n",
      "Prediction: 0.43,    Actual Value: 0.53\n",
      "Prediction: 0.43,    Actual Value: 0.52\n",
      "Prediction: 0.45,    Actual Value: 0.48\n",
      "Prediction: 0.43,    Actual Value: 0.48\n",
      "Prediction: 0.51,    Actual Value: 0.48\n",
      "Prediction: 0.57,    Actual Value: 0.52\n",
      "Prediction: 0.51,    Actual Value: 0.45\n",
      "Prediction: 0.56,    Actual Value: 0.46\n",
      "Prediction: 0.51,    Actual Value: 0.44\n",
      "Prediction: 0.53,    Actual Value: 0.53\n",
      "Prediction: 0.62,    Actual Value: 0.47\n",
      "Prediction: 0.52,    Actual Value: 0.46\n",
      "Prediction: 0.48,    Actual Value: 0.52\n",
      "Prediction: 0.58,    Actual Value: 0.52\n",
      "Prediction: 0.44,    Actual Value: 0.52\n",
      "Prediction: 0.46,    Actual Value: 0.50\n",
      "Prediction: 0.51,    Actual Value: 0.50\n",
      "Prediction: 0.47,    Actual Value: 0.57\n",
      "Prediction: 0.61,    Actual Value: 0.57\n",
      "Prediction: 0.60,    Actual Value: 0.69\n",
      "Prediction: 0.65,    Actual Value: 0.72\n",
      "Prediction: 0.61,    Actual Value: 0.74\n",
      "Prediction: 0.66,    Actual Value: 0.79\n",
      "Prediction: 0.62,    Actual Value: 0.79\n",
      "Prediction: 0.71,    Actual Value: 0.79\n",
      "Prediction: 0.65,    Actual Value: 0.79\n",
      "Prediction: 0.66,    Actual Value: 0.77\n",
      "Prediction: 0.61,    Actual Value: 0.74\n",
      "Prediction: 0.46,    Actual Value: 0.78\n",
      "Prediction: 0.50,    Actual Value: 0.54\n",
      "Prediction: 0.51,    Actual Value: 0.55\n",
      "Prediction: 0.49,    Actual Value: 0.41\n",
      "Prediction: 0.49,    Actual Value: 0.36\n",
      "Prediction: 0.63,    Actual Value: 0.62\n",
      "Prediction: 0.78,    Actual Value: 0.81\n",
      "Prediction: 0.78,    Actual Value: 0.86\n",
      "Prediction: 0.72,    Actual Value: 0.86\n",
      "Prediction: 0.80,    Actual Value: 0.85\n",
      "Prediction: 0.86,    Actual Value: 0.95\n",
      "Prediction: 0.93,    Actual Value: 0.95\n",
      "Prediction: 0.83,    Actual Value: 0.95\n",
      "Prediction: 0.89,    Actual Value: 0.98\n",
      "Prediction: 0.90,    Actual Value: 0.98\n",
      "Prediction: 0.79,    Actual Value: 0.85\n",
      "Prediction: 0.77,    Actual Value: 0.61\n",
      "Prediction: 0.74,    Actual Value: 0.65\n",
      "Prediction: 0.73,    Actual Value: 0.65\n",
      "Prediction: 0.73,    Actual Value: 0.61\n",
      "Prediction: 0.72,    Actual Value: 0.54\n",
      "Prediction: 0.70,    Actual Value: 0.50\n",
      "Prediction: 0.67,    Actual Value: 0.50\n",
      "Prediction: 0.61,    Actual Value: 0.57\n",
      "Prediction: 0.65,    Actual Value: 0.59\n",
      "Prediction: 0.66,    Actual Value: 0.57\n",
      "Prediction: 0.66,    Actual Value: 0.52\n",
      "Prediction: 0.62,    Actual Value: 0.42\n",
      "Prediction: 0.67,    Actual Value: 0.45\n",
      "Prediction: 0.65,    Actual Value: 0.45\n",
      "Prediction: 0.68,    Actual Value: 0.48\n",
      "Prediction: 0.67,    Actual Value: 0.59\n",
      "Prediction: 0.65,    Actual Value: 0.66\n",
      "Prediction: 0.69,    Actual Value: 0.67\n",
      "Prediction: 0.69,    Actual Value: 0.74\n",
      "Prediction: 0.78,    Actual Value: 0.79\n",
      "Prediction: 0.80,    Actual Value: 0.94\n",
      "Prediction: 0.85,    Actual Value: 0.89\n",
      "Prediction: 0.83,    Actual Value: 0.99\n",
      "Prediction: 0.92,    Actual Value: 0.95\n",
      "Prediction: 0.92,    Actual Value: 0.95\n",
      "Prediction: 0.99,    Actual Value: 0.95\n",
      "Prediction: 0.99,    Actual Value: 0.97\n",
      "Prediction: 1.02,    Actual Value: 1.01\n",
      "Prediction: 1.00,    Actual Value: 0.94\n",
      "Prediction: 1.00,    Actual Value: 0.92\n",
      "Prediction: 1.01,    Actual Value: 0.74\n",
      "Prediction: 1.05,    Actual Value: 1.21\n",
      "Prediction: 1.11,    Actual Value: 1.24\n",
      "Prediction: 1.10,    Actual Value: 1.24\n",
      "Prediction: 1.10,    Actual Value: 1.24\n",
      "Prediction: 1.10,    Actual Value: 1.23\n",
      "Prediction: 1.11,    Actual Value: 1.21\n",
      "Prediction: 1.15,    Actual Value: 1.30\n",
      "Prediction: 1.09,    Actual Value: 1.28\n",
      "Prediction: 1.05,    Actual Value: 1.27\n",
      "Prediction: 0.98,    Actual Value: 1.26\n",
      "Prediction: 0.85,    Actual Value: 0.66\n",
      "Prediction: 0.81,    Actual Value: 0.61\n",
      "Prediction: 0.78,    Actual Value: 0.57\n",
      "Prediction: 0.73,    Actual Value: 0.64\n",
      "Prediction: 0.68,    Actual Value: 0.63\n",
      "Prediction: 0.66,    Actual Value: 0.67\n",
      "Prediction: 0.68,    Actual Value: 0.54\n",
      "Prediction: 0.66,    Actual Value: 0.54\n",
      "Prediction: 0.62,    Actual Value: 0.53\n",
      "Prediction: 0.64,    Actual Value: 0.53\n",
      "Prediction: 0.67,    Actual Value: 0.79\n",
      "Prediction: 0.73,    Actual Value: 0.80\n",
      "Prediction: 0.79,    Actual Value: 0.85\n",
      "Prediction: 0.83,    Actual Value: 0.85\n",
      "Prediction: 0.84,    Actual Value: 0.92\n",
      "Prediction: 0.82,    Actual Value: 0.97\n",
      "Prediction: 0.85,    Actual Value: 0.88\n",
      "Prediction: 0.83,    Actual Value: 0.87\n",
      "Prediction: 0.83,    Actual Value: 0.88\n",
      "Prediction: 0.78,    Actual Value: 0.89\n",
      "Prediction: 0.72,    Actual Value: 0.70\n",
      "Prediction: 0.65,    Actual Value: 0.70\n",
      "Prediction: 0.60,    Actual Value: 0.49\n",
      "Prediction: 0.55,    Actual Value: 0.44\n",
      "Prediction: 0.58,    Actual Value: 0.40\n",
      "Prediction: 0.57,    Actual Value: 0.33\n",
      "Prediction: 0.57,    Actual Value: 0.33\n",
      "Prediction: 0.57,    Actual Value: 0.24\n",
      "Prediction: 0.61,    Actual Value: 0.65\n",
      "Prediction: 0.65,    Actual Value: 0.65\n",
      "Prediction: 0.69,    Actual Value: 0.62\n",
      "Prediction: 0.74,    Actual Value: 0.58\n",
      "Prediction: 0.78,    Actual Value: 0.70\n",
      "Prediction: 0.79,    Actual Value: 0.68\n",
      "Prediction: 0.83,    Actual Value: 0.77\n",
      "Prediction: 0.89,    Actual Value: 0.90\n",
      "Prediction: 0.86,    Actual Value: 0.91\n",
      "Prediction: 0.81,    Actual Value: 0.91\n",
      "Prediction: 0.76,    Actual Value: 0.64\n",
      "Prediction: 0.72,    Actual Value: 0.61\n",
      "Prediction: 0.67,    Actual Value: 0.58\n",
      "Prediction: 0.64,    Actual Value: 0.53\n"
     ]
    }
   ],
   "source": [
    "for ind, i in enumerate(lstm.predict(X_test)):\n",
    "    \n",
    "    print('Prediction: ' + str('{:.2f}'.format(round(100 * round(i[0], 4),3))) + ',    ' + 'Actual Value: ' + str('{:.2f}'.format(round(100 * round(y_test[ind][0],4),2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the results of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_out_results_of_a_model(model,X_test,y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print the R2 score \n",
    "\n",
    "    print (\"R2 score:\\n\") \n",
    "    print (('{:.2f}'.format((100*(r2_score(y_test, y_pred))))) + \" %\")\n",
    "\n",
    "    print (\"\\n\")\n",
    "    \n",
    "    # Print the RMSE\n",
    "\n",
    "    print (\"RMSE:\\n\")\n",
    "    print (math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    print ('\\n')\n",
    "    \n",
    "    # Print the mean squared error\n",
    "    \n",
    "    print (\"Mean Squared Error:\\n\")\n",
    "    print (mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score:\n",
      "\n",
      "84.47 %\n",
      "\n",
      "\n",
      "RMSE:\n",
      "\n",
      "0.0011216986411707879\n",
      "\n",
      "\n",
      "Mean Squared Error:\n",
      "\n",
      "1.258207841604392e-06\n"
     ]
    }
   ],
   "source": [
    "printing_out_results_of_a_model(lstm, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
